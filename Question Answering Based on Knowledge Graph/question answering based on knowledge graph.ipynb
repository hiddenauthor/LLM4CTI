{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read file from RQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pickle, os, json, re, time, random, logging, pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, scipy, sklearn, networkx as nx, importlib\n",
    "import tools\n",
    "importlib.reload(tools)\n",
    "\n",
    "# Read the Excel file from the specified path\n",
    "df = pd.read_excel('df.xlsx')\n",
    "\n",
    "# Display the column names\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_prompt(input_details):\n",
    "    \"\"\"\n",
    "    Generate single-choice questions based on the knowledge graph or full-text content.\n",
    "    External background knowledge is forbidden; only information inferable from the input may be used.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    # Extract entity names from the input for setting the question target\n",
    "    entity_match = re.search(r'entities are \\[(.*?)\\]', input_details)\n",
    "    entities = entity_match.group(1).split(',') if entity_match else []\n",
    "    \n",
    "    if len(entities) > 1:\n",
    "        entity_requirement = (\n",
    "            f\"The questions must cover all of the listed cyber threat-related entities: {', '.join(entities)}. \"\n",
    "            \"Questions should reflect each entity's specific roles, actions, and relationships as shown in the provided content.\"\n",
    "        )\n",
    "    elif len(entities) == 1:\n",
    "        entity_requirement = (\n",
    "            f\"The questions must focus on the specified cyber threat entity: {entities[0]}. \"\n",
    "            \"The questions should analyze its behaviors, characteristics, and how it interacts with other entities based solely on the input.\"\n",
    "        )\n",
    "    else:\n",
    "        entity_requirement = (\n",
    "            \"The questions must target cyber threat-related entities discussed in the content. \"\n",
    "            \"Each question should focus on an identifiable threat-related behavior or relationship.\"\n",
    "        )\n",
    "\n",
    "    prompt_message = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a large language model that must rely solely on the provided threat intelligence report or structured knowledge graph as your only source of information. \"\n",
    "                \"You do NOT have access to any external world knowledge, pretrained threat data, or background about the entities. \"\n",
    "                \"You are strictly forbidden from assuming any facts not explicitly mentioned or inferable from the provided input. \"\n",
    "                \"If certain information is not found within the input, you must not guess, generalize, or rely on external cybersecurity knowledge.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"The following content contains threat intelligence data (either full-text article or structured knowledge graph). \"\n",
    "                \"All questions and answers must be exclusively answerable based on this content. \"\n",
    "                \"You MUST NOT incorporate any outside knowledge about cyber threats or general attack techniques. \"\n",
    "                \"The correct answers must be supported by specific evidence in the input content.\\n\\n\"\n",
    "                \"Here is the input:\\n\"\n",
    "                + input_details +\n",
    "                \"\\n\\n\"\n",
    "                \"Now, please generate 10 CHALLENGING single-choice questions, strictly satisfying the following conditions:\\n\"\n",
    "                \"0. Each question should mainly focus on the topic entity of the content that I provided. \"\n",
    "                \"The question text should contain the topic entity name and inquire about its behavior on computer systems, networks, or data, its built-in functions/characteristics, or its relationship with other entities.\\n\"\n",
    "                \"1. The answer should not be guessable by only reading the question and the four answer choices. \\n\"\n",
    "                \"2. Skip any questions about who discovered the topic, which company discovered it first, or which solution/software can solve the topic.\\n\"\n",
    "                \"3. The correct answer must be clearly supported by the content.\\n\"\n",
    "                \"4. Each question must have four answer choices: A, B, C, and D.\\n\"\n",
    "                \"   - One answer should be a 'wrong name' that appears in the content but is not supported as the correct answer. It should not be the same as the 'correct answer' or 'similar entity name'.\\n\"\n",
    "                \"   - Another answer should be a 'similar entity name' modified from the 'correct answer' or the 'wrong name' using synonyms/antonyms/meaningful word changes (for example, changing 'Expatriate Pakistanis' to 'Local Pakistanis'). Avoid trivial letter or spelling changes (such as 'Expatriate Pakistans' or 'Expatriate Pakistonis').\\n\"\n",
    "                \"   - One answer is the correct answer.\\n\"\n",
    "                \"   - The last answer should be a 'random name' that does not appear in the content and appears plausible if a reader only has access to the question and its four answer choices.\\n\"\n",
    "                \"5. You must return a JSON array of **exactly 10** question objects with no extra explanations or commentary.\\n\"\n",
    "                \"6. Each question must be in the following format:\\n\"\n",
    "                \"{\\n\"\n",
    "                '  \"raw_question\": \"<One challenging question that requires understanding of the input content>\\\\nA. ...\\\\nB. ...\\\\nC. ...\\\\nD. ...\",\\n'\n",
    "                '  \"correct_answer\": \"C\"\\n'\n",
    "                \"}\\n\\n\"\n",
    "                \"If the input lacks sufficient details to create 10 valid questions, state so explicitly and return an empty JSON array: []\\n\\n\"\n",
    "                + entity_requirement\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return prompt_message\n",
    "\n",
    "\n",
    "# Initialize a list to store all input contents\n",
    "alltext = []\n",
    "\n",
    "# For each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Extract the target entity for question generation and the full text\n",
    "    target_entities = row[\"Topic Threat\"]\n",
    "    target_text = row['text']\n",
    "    # Construct the input content string\n",
    "    input_content = 'Topic entity of the content are: ' + str(target_entities) + ' and the content is:' + str(target_text)\n",
    "    # Append to the list\n",
    "    alltext.append(input_content)\n",
    "    \n",
    "print(f\"Number of generated input content: {len(alltext)}\")\n",
    "\n",
    "# Process all input content with prompt_list_maker from the tools module\n",
    "prompts = tools.prompt_list_maker(\n",
    "    alltext,\n",
    "    prompt_maker_fun=generate_question_prompt,\n",
    "    max_length_inK_of_prompt=128,\n",
    "    min_length_of_prompt=128,\n",
    "    count_in_tokenlen=True,\n",
    "    sample_size=100\n",
    ")\n",
    "\n",
    "ans_questions = tools.ask_group_link(\n",
    "    prompts,\n",
    "    token=89 * 1024,\n",
    "    temp=1,\n",
    "    model=\"gpto3mini\",\n",
    "    streamprint=False,\n",
    "    max_workers=32,\n",
    "    weight=2,\n",
    "    forcegpt=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create df_ai for RQ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from json_repair import repair_json\n",
    "from json_repair import loads as json_loads\n",
    "\n",
    "def simplify_kg_str(inputstring: str) -> str:\n",
    "    \"\"\"\n",
    "    Simplify the input knowledge graph string:\n",
    "      - Extract the entity list and relationship list from the input string (delimited by specific markers).\n",
    "      - For entities: output only the entity names that have non-empty alias or mother entity information (in a simplified format).\n",
    "      - For relationships: format as triples [sub, rel, obj].\n",
    "    If the simplified relationship string is less than 10 characters in length, return the original string.\n",
    "    \"\"\"\n",
    "    # Define regex patterns to extract entity and relationship sections\n",
    "    entity_pattern = r\"#Final_Entity_List_Start#\\s*json\\s*(\\[[\\s\\S]*?\\])\\s*#Final_Entity_List_End#\"\n",
    "    relationship_pattern = r\"#Final_Relationship_List_Start#\\s*json\\s*(\\[[\\s\\S]*?\\])\\s*#Final_Relationship_List_End#\"\n",
    "    \n",
    "    # Parse the entity list\n",
    "    entity_match = re.search(entity_pattern, inputstring)\n",
    "    if entity_match:\n",
    "        entity_json_str = entity_match.group(1)\n",
    "        try:\n",
    "            EntityList = json_loads(entity_json_str)\n",
    "        except Exception:\n",
    "            EntityList = []\n",
    "    else:\n",
    "        EntityList = []\n",
    "    \n",
    "    # Parse the relationship list\n",
    "    relationship_match = re.search(relationship_pattern, inputstring)\n",
    "    if relationship_match:\n",
    "        relationship_json_str = relationship_match.group(1)\n",
    "        try:\n",
    "            RelationshipList = json_loads(relationship_json_str)\n",
    "        except Exception:\n",
    "            RelationshipList = []\n",
    "    else:\n",
    "        RelationshipList = []\n",
    "    \n",
    "    # Simplify entities: keep only those with non-empty alias or mother entity\n",
    "    simplified_entities = []\n",
    "    for entity in EntityList:\n",
    "        if isinstance(entity, dict):\n",
    "            name = entity.get(\"name\", \"\").strip()\n",
    "            alias_list = [a for a in entity.get(\"alias\", []) if a and a != \"None\"]\n",
    "            mother_list = [m for m in entity.get(\"mother entity\", []) if m and m != \"None\"]\n",
    "            # Process only if alias_list or mother_list is not empty\n",
    "            if alias_list or mother_list:\n",
    "                extras = []\n",
    "                if alias_list:\n",
    "                    extras.append(\"alias:[\" + \",\".join(alias_list) + \"]\")\n",
    "                if mother_list:\n",
    "                    extras.append(\"mother:[\" + \",\".join(mother_list) + \"]\")\n",
    "                extra = \"(\" + \",\".join(extras) + \")\" if extras else \"\"\n",
    "                if name:\n",
    "                    simplified_entities.append(name + extra)\n",
    "    \n",
    "    # Simplify relationships: format as [sub, rel, obj] triple\n",
    "    simplified_relationships = []\n",
    "    for rel in RelationshipList:\n",
    "        if isinstance(rel, dict):\n",
    "            sub = rel.get(\"sub\", \"\").strip()\n",
    "            relation = rel.get(\"rel\", \"\").strip()\n",
    "            obj = rel.get(\"obj\", \"\").strip()\n",
    "            if sub and relation and obj:\n",
    "                simplified_relationships.append(\"[\" + \",\".join([sub, relation, obj]) + \"]\")\n",
    "    \n",
    "    # Combine the relationships into a single string\n",
    "    relationship_str = \",\".join(simplified_relationships)\n",
    "    \n",
    "    # If the relationship string is too short, return the original input string\n",
    "    if len(relationship_str) < 10:\n",
    "        return inputstring\n",
    "    \n",
    "    return (\n",
    "        \"Special node with alias and mother entity: \" + \",\".join(simplified_entities)\n",
    "        + \"\\nFull knowledge graph: \" + relationship_str\n",
    "    )\n",
    "    \n",
    "def simplify_kg_str_for_4o_and_o3(inputstring: str) -> str:\n",
    "    \"\"\"\n",
    "    Simplify the input knowledge graph string suitable for 4o and o3 formats:\n",
    "      - Extract the entity list and relationship list from the input string using the following markers:\n",
    "            #Entity_List_Start# ... #Entity_List_End#\n",
    "            #Relationship_List_Start# ... #Relationship_List_End#\n",
    "        Supports optional markdown code block wrappers (e.g., ```json ... ```).\n",
    "      - For entities: output the entity name, appending non-empty alias and mother entity information.\n",
    "      - For relationships: format as a triple [sub, rel, obj] (convert non-string fields to string).\n",
    "    \n",
    "    Return the final descriptive string.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from json_repair import loads as json_loads\n",
    "\n",
    "    def safe_strip(value):\n",
    "        if isinstance(value, str):\n",
    "            return value.strip()\n",
    "        elif isinstance(value, list):\n",
    "            # Convert each element in the list to string and join with commas\n",
    "            return \",\".join(str(item).strip() for item in value)\n",
    "        else:\n",
    "            return str(value).strip()\n",
    "\n",
    "    # Regex patterns supporting markdown code block wrappers\n",
    "    entity_pattern = r\"#Entity_List_Start#\\s*(?:```json\\s*)?(\\[[\\s\\S]*?\\])(?:\\s*```)?\\s*#Entity_List_End#\"\n",
    "    relationship_pattern = r\"#Relationship_List_Start#\\s*(?:```json\\s*)?(\\[[\\s\\S]*?\\])(?:\\s*```)?\\s*#Relationship_List_End#\"\n",
    "    \n",
    "    # Parse the entity list\n",
    "    EntityList = []\n",
    "    entity_match = re.search(entity_pattern, inputstring)\n",
    "    if entity_match:\n",
    "        entity_json_str = entity_match.group(1)\n",
    "        try:\n",
    "            EntityList = json_loads(entity_json_str)\n",
    "        except Exception:\n",
    "            EntityList = []\n",
    "    \n",
    "    # Parse the relationship list\n",
    "    RelationshipList = []\n",
    "    rel_match = re.search(relationship_pattern, inputstring)\n",
    "    if rel_match:\n",
    "        relationship_json_str = rel_match.group(1)\n",
    "        try:\n",
    "            RelationshipList = json_loads(relationship_json_str)\n",
    "        except Exception:\n",
    "            RelationshipList = []\n",
    "    \n",
    "    # Simplify entities: output name with non-empty alias and mother entity information\n",
    "    simplified_entities = []\n",
    "    for entity in EntityList:\n",
    "        if isinstance(entity, dict):\n",
    "            name = safe_strip(entity.get(\"name\", \"\"))\n",
    "            alias_list = entity.get(\"alias\", [])\n",
    "            alias_list = [safe_strip(a) for a in alias_list if a and a != \"None\"]\n",
    "            mother_list = entity.get(\"mother entity\", [])\n",
    "            mother_list = [safe_strip(m) for m in mother_list if m and m != \"None\"]\n",
    "            extras = []\n",
    "            if alias_list:\n",
    "                extras.append(\"alias:[\" + \",\".join(alias_list) + \"]\")\n",
    "            if mother_list:\n",
    "                extras.append(\"mother:[\" + \",\".join(mother_list) + \"]\")\n",
    "            extra = \"(\" + \",\".join(extras) + \")\" if extras else \"\"\n",
    "            if name:\n",
    "                simplified_entities.append(name + extra)\n",
    "    \n",
    "    # Simplify relationships: format as [sub, rel, obj] triple\n",
    "    simplified_relationships = []\n",
    "    for rel in RelationshipList:\n",
    "        if isinstance(rel, dict):\n",
    "            sub = safe_strip(rel.get(\"sub\", \"\"))\n",
    "            relation = safe_strip(rel.get(\"rel\", \"\"))\n",
    "            obj = safe_strip(rel.get(\"obj\", \"\"))\n",
    "            if sub and relation and obj:\n",
    "                simplified_relationships.append(\"[\" + \",\".join([sub, relation, obj]) + \"]\")\n",
    "    \n",
    "    return (\n",
    "        \"Special node with alias and mother entity: \" + \", \".join(simplified_entities) +\n",
    "        \"\\nFull knowledge graph: \" + \", \".join(simplified_relationships)\n",
    "    )\n",
    "\n",
    "# Build rows for the final DataFrame\n",
    "rows = []\n",
    "# Original method list\n",
    "methods = ['CyberDoc', 'GPT4o', 'CTIKG']\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # Extract 'Article' (正文) and 'idorurl'\n",
    "    text = df.iloc[i]['text']\n",
    "    idorurl = df.iloc[i]['idorurl']\n",
    "    \n",
    "    # Parse ans_quesitons[i] (a string). Try using json.loads first; if fails, repair using repair_json.\n",
    "    try:\n",
    "        questions = json.loads(ans_questions[i])\n",
    "    except Exception:\n",
    "        try:\n",
    "            fixed = repair_json(ans_questions[i])\n",
    "            questions = json.loads(fixed)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing ans_questions[{i}]: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not isinstance(questions, list):\n",
    "        print(f\"ans_questions[{i}] did not parse as a list\")\n",
    "        continue\n",
    "\n",
    "    # For each question, generate 5 entries corresponding to the original methods and two additional methods.\n",
    "    for q in questions:\n",
    "        if isinstance(q, dict) and 'raw_question' in q and 'correct_answer' in q:\n",
    "            raw_q = q['raw_question']\n",
    "            # Retain the complete raw_question (including ABCD options), remove numbering at the beginning\n",
    "            question_text = re.sub(r'^\\d+\\.\\s*', '', raw_q.strip())\n",
    "            answer_text = q['correct_answer']\n",
    "        else:\n",
    "            question_text = str(q).strip()\n",
    "            answer_text = \"\"\n",
    "        \n",
    "        # For the original methods\n",
    "        for method in methods:\n",
    "            # Choose KG string simplification based on the method:\n",
    "            # For 'CyberDoc', use simplify_kg_str; for 'GPT4o' and 'Correct', use simplify_kg_str_for_4o_and_o3.\n",
    "            if method in ['CyberDoc']:\n",
    "                method_cell = df.iloc[i][f'{method}结果']\n",
    "                method_result = simplify_kg_str(method_cell)\n",
    "            elif method in ['GPT4o', 'Correct']:\n",
    "                method_cell = df.iloc[i][f'{method}结果']\n",
    "                method_result = simplify_kg_str_for_4o_and_o3(method_cell)\n",
    "            else:\n",
    "                method_result = df.iloc[i][f'{method}结果']\n",
    "            rows.append([\n",
    "                text,             # Article\n",
    "                question_text,    # Question\n",
    "                answer_text,      # Correct Answer\n",
    "                method,           # Method\n",
    "                method_result,    # Context provided by the method\n",
    "                idorurl           # idorurl\n",
    "            ])\n",
    "        rows.append([\n",
    "            text,             # Article\n",
    "            question_text,    # Question\n",
    "            answer_text,      # Correct Answer\n",
    "            \"No Content\",     # Method\n",
    "            \"Sorry, No Context is available for this question, you should provide a final answer by your own\",  # Fixed prompt\n",
    "            idorurl           # idorurl\n",
    "        ])\n",
    "\n",
    "# Create the DataFrame and rename columns to English\n",
    "df_ai = pd.DataFrame(rows, columns=['Article', 'Question', 'Correct Answer', 'Method', 'Method Context', 'idorurl'])\n",
    "print(f\"Created {len(df_ai)} rows of data\")\n",
    "df_ai.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ask questions based on graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(article, original_question):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"The article or the related knowledge graph is provided below:\\n\" + article + \"\\n\\n\" +\n",
    "                \"Here is the question:\\n\" + original_question + \"\\n\\n\"\n",
    "                \"Your answer should be in this format: <think>How to answer the question</think>Answer (one character A/B/C/D), such as '<think>The question is about the behavior of the entity, so I find that the content ''xxx'' may solve the problem</think>A'.\\n\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    return prompt\n",
    "\n",
    "# Two lists to store prompts and corresponding df_ai indices so that answers can be mapped back later.\n",
    "qwq_prompts = []\n",
    "fouro_prompts = []\n",
    "qwq_indices = []\n",
    "fouro_indices = []\n",
    "\n",
    "# Iterate through df_ai to generate prompts and categorize based on the method.\n",
    "for idx in df_ai.index:\n",
    "    article = df_ai.loc[idx, 'Method Context']\n",
    "    # Use the \"Question\" field in df_ai as the original question\n",
    "    original_question = df_ai.loc[idx, 'Question']\n",
    "    prompt = generate_prompt(article, original_question)\n",
    "    if df_ai.loc[idx, 'Method'] == 'CyberDoc':\n",
    "        qwq_prompts.append(prompt)\n",
    "        qwq_indices.append(idx)\n",
    "    else:\n",
    "        fouro_prompts.append(prompt)\n",
    "        fouro_indices.append(idx)\n",
    "\n",
    "# Call the ask_batch API for each group\n",
    "answer_qwq = tools.ask_batch(\n",
    "    qwq_prompts,\n",
    "    token=16 * 1024,\n",
    "    temp=0.7,\n",
    "    model=\"gpt4omini\",\n",
    "    streamprint=False,\n",
    "    max_workers=12,\n",
    "    weight='auto',\n",
    "    forcegpt=True\n",
    ")\n",
    "\n",
    "answer_gpt = tools.ask_batch(\n",
    "    fouro_prompts,\n",
    "    token=16 * 1024,\n",
    "    temp=0.7,\n",
    "    model=\"gpt4omini\",\n",
    "    streamprint=False,\n",
    "    max_workers=12,\n",
    "    weight='auto',\n",
    "    forcegpt=True\n",
    ")\n",
    "\n",
    "# Map the returned answers back to df_ai in the \"Context-based Answer\" column.\n",
    "answer_qwq_cleaned = tools.cleanthinkans(answer_qwq)\n",
    "answer_gpt_cleaned = tools.cleanthinkans(answer_gpt)\n",
    "\n",
    "import re\n",
    "# Define a function to extract the first option letter (A/B/C/D) from an answer.\n",
    "def extract_first_option(answer):\n",
    "    match = re.search(r'[ABCD]', answer)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return answer\n",
    "\n",
    "# Process answers from the QWQ model.\n",
    "for idx, ans in zip(qwq_indices, answer_qwq_cleaned):\n",
    "    option = extract_first_option(ans)\n",
    "    df_ai.at[idx, 'Context-based Answer'] = option\n",
    "\n",
    "# Process answers from the GPT model.\n",
    "for idx, ans in zip(fouro_indices, answer_gpt_cleaned):\n",
    "    option = extract_first_option(ans)\n",
    "    df_ai.at[idx, 'Context-based Answer'] = option\n",
    "\n",
    "print(\"Q&A complete, and the context-based answers (options A/B/C/D) have been mapped to df_ai.\")\n",
    "\n",
    "# Compare \"Correct Answer\" and \"Context-based Answer\" to generate a new column \"Is Correct\"\n",
    "df_ai['Is Correct'] = df_ai['Correct Answer'] == df_ai['Context-based Answer']\n",
    "\n",
    "# Group by \"Method\" to compute total counts and correct counts, then calculate accuracy.\n",
    "accuracy_by_method = df_ai.groupby('Method')['Is Correct'].agg(Total='count', Correct='sum')\n",
    "accuracy_by_method['Accuracy'] = accuracy_by_method['Correct'] / accuracy_by_method['Total']\n",
    "\n",
    "print(accuracy_by_method)\n",
    "\n",
    "# Create a mapping from 'idorurl' to the main category from the original df.\n",
    "classification_map = df.set_index('idorurl')['article type'].to_dict()\n",
    "\n",
    "# Add a new column \"Major Category\" to df_ai using the mapping from idorurl.\n",
    "df_ai['Major Category'] = df_ai['idorurl'].map(classification_map)\n",
    "\n",
    "# Group by \"Method\" and \"Major Category\" to calculate accuracy.\n",
    "accuracy_by_method_and_class = df_ai.groupby(['Method', 'Major Category'])['Is Correct'].agg(\n",
    "    Total='count', Correct='sum'\n",
    ")\n",
    "accuracy_by_method_and_class['Accuracy'] = accuracy_by_method_and_class['Correct'] / accuracy_by_method_and_class['Total']\n",
    "\n",
    "# Create a pivot table for better visualization.\n",
    "pivot_accuracy = accuracy_by_method_and_class['Accuracy'].unstack('Major Category')\n",
    "\n",
    "# Calculate the average accuracy for each method.\n",
    "pivot_accuracy['Average Accuracy'] = df_ai.groupby('Method')['Is Correct'].mean()\n",
    "\n",
    "# Rename axes for a professional presentation.\n",
    "pivot_accuracy.rename_axis(\"Threat Category\", axis=\"columns\", inplace=True)\n",
    "pivot_accuracy.rename_axis(\"Method\", axis=\"index\", inplace=True)\n",
    "\n",
    "# Format accuracy as percentages with two decimal places.\n",
    "pivot_accuracy = pivot_accuracy.applymap(lambda x: f\"{x*100:.2f}%\" if pd.notnull(x) else \"N/A\")\n",
    "\n",
    "# Reorder methods as specified.\n",
    "method_order = ['CyberDoc', 'CTIKG', 'GPT4o', 'No Content']\n",
    "pivot_accuracy = pivot_accuracy.reindex(method_order)\n",
    "\n",
    "# Display the final results.\n",
    "print(\"\\nAccuracy for Different Threat Categories:\")\n",
    "print(pivot_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
