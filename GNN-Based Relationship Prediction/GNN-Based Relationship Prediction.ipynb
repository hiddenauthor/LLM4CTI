{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knowledge_graph(df, index, kg_col, keyindex_col):\n",
    "    \"\"\"\n",
    "    Build a knowledge graph from a specified row of the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "      df         : pandas DataFrame containing the knowledge graph string.\n",
    "      index      : index of the row to process.\n",
    "      kg_col     : column name in the DataFrame that contains the KG string.\n",
    "      keyindex_col: column name used to index (privacy content removed).\n",
    "      \n",
    "    Returns:\n",
    "      G          : A networkx MultiDiGraph constructed from the extracted data.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import re\n",
    "    import networkx as nx\n",
    "    import json_repair\n",
    "    from collections import Counter\n",
    "    import ast\n",
    "\n",
    "    # Get the knowledge graph string from the specified row\n",
    "    data_str = df.loc[index, kg_col]\n",
    "    # Note: Privacy information from keyindex is not extracted and used anymore.\n",
    "\n",
    "    # 1. Extract JSON string for the entity list using regex.\n",
    "    entity_pattern = r'#Final_Entity_List_Start#\\s*json\\s*(\\[[\\s\\S]*?\\])\\s*#Final_Entity_List_End#'\n",
    "    entity_match = re.search(entity_pattern, data_str)\n",
    "    if entity_match:\n",
    "        entity_json_str = entity_match.group(1)\n",
    "        try:\n",
    "            entities = json_repair.loads(entity_json_str)\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing entity JSON:\", e)\n",
    "            entities = []\n",
    "    else:\n",
    "        entities = []\n",
    "\n",
    "    # 2. Extract JSON string for the relationship list using regex.\n",
    "    rel_pattern = r'#Final_Relationship_List_Start#\\s*json\\s*(\\[[\\s\\S]*?\\])\\s*#Final_Relationship_List_End#'\n",
    "    rel_match = re.search(rel_pattern, data_str)\n",
    "    if rel_match:\n",
    "        rel_json_str = rel_match.group(1)\n",
    "        try:\n",
    "            relationships = json_repair.loads(rel_json_str)\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing relationship JSON:\", e)\n",
    "            relationships = []\n",
    "    else:\n",
    "        relationships = []\n",
    "\n",
    "    # 3. Create a MultiDiGraph\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    # 4. Add entities as nodes (using \"name\" as the node identifier)\n",
    "    for i, entity in enumerate(entities):\n",
    "        node_id = entity.get(\"name\")\n",
    "        if node_id is None:\n",
    "            print(f\"Warning: Entity number {i} is missing 'name' field. Skipping entity. Content: {entity}\")\n",
    "            continue\n",
    "        # Privacy: removed addition of private info.\n",
    "        G.add_node(node_id, **entity)\n",
    "\n",
    "    # 5. Process relationships and add them as edges.\n",
    "    for rel in relationships:\n",
    "        # If relationship is a string, try converting it to a dict.\n",
    "        if isinstance(rel, str):\n",
    "            try:\n",
    "                rel = ast.literal_eval(rel)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing relationship {rel}: {e}\")\n",
    "                continue\n",
    "        source = rel.get(\"sub\")\n",
    "        target = rel.get(\"obj\")\n",
    "        if source is None or target is None:\n",
    "            print(\"Warning: Relationship missing 'sub' or 'obj' field. Skipping relationship.\", rel)\n",
    "            continue\n",
    "        # Exclude 'sub' and 'obj' from edge attributes.\n",
    "        edge_attr = {key: value for key, value in rel.items() if key not in [\"sub\", \"obj\"]}\n",
    "        # Privacy: removed addition of private info.\n",
    "        G.add_edge(source, target, **edge_attr)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def aggregate_knowledge_graph(df, kg_col, keyindex_col):\n",
    "    \"\"\"\n",
    "    Aggregate knowledge graphs from all rows of the DataFrame into one graph.\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "    import spacy\n",
    "\n",
    "    # Load spaCy English model.\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def spacy_lemmatize(text):\n",
    "        \"\"\"\n",
    "        Convert text to lowercase and lemmatize it using spaCy.\n",
    "        For example, 'Social Engineering Attacks' becomes 'social engineering attack'\n",
    "        \"\"\"\n",
    "        doc = nlp(text.lower())\n",
    "        return \" \".join(token.lemma_ for token in doc if not token.is_punct and not token.is_space)\n",
    "\n",
    "    # Helper function: merge two values into a list (preserving order and deduplication).\n",
    "    def merge_values(val1, val2):\n",
    "        def to_list(x):\n",
    "            if x is None:\n",
    "                return []\n",
    "            if isinstance(x, list):\n",
    "                return x\n",
    "            return [x]\n",
    "        list1 = to_list(val1)\n",
    "        list2 = to_list(val2)\n",
    "        merged = list(dict.fromkeys(list1 + list2))\n",
    "        return merged\n",
    "\n",
    "    # 1) First, merge all the knowledge graphs from each row.\n",
    "    G_total = nx.MultiDiGraph()\n",
    "    for index in df.index:\n",
    "        G = build_knowledge_graph(df, index, kg_col, keyindex_col)\n",
    "        # Merge nodes: if a node exists, merge its attributes.\n",
    "        for node, attr in G.nodes(data=True):\n",
    "            if node not in G_total:\n",
    "                G_total.add_node(node, **attr)\n",
    "            else:\n",
    "                for key in set(list(attr.keys()) + list(G_total.nodes[node].keys())):\n",
    "                    val_new = attr.get(key)\n",
    "                    val_old = G_total.nodes[node].get(key)\n",
    "                    G_total.nodes[node][key] = merge_values(val_old, val_new)\n",
    "        # Merge all edges (allowing multiple edges).\n",
    "        for u, v, edge_attr in G.edges(data=True):\n",
    "            G_total.add_edge(u, v, **edge_attr)\n",
    "\n",
    "    # 2) Use spaCy to lemmatize node names and group nodes by their lemma.\n",
    "    lemma_to_nodes = {}\n",
    "    for node in list(G_total.nodes()):\n",
    "        if isinstance(node, str):\n",
    "            lemma = spacy_lemmatize(node)\n",
    "        else:\n",
    "            lemma = node\n",
    "        lemma_to_nodes.setdefault(lemma, []).append(node)\n",
    "    \n",
    "    duplicate_groups_count = sum(1 for group in lemma_to_nodes.values() if len(group) > 1)\n",
    "    print(\"There are {} groups of nodes with identical lemmas that need to be merged.\".format(duplicate_groups_count))\n",
    "    \n",
    "    # 3) For each group with more than one node, merge them.\n",
    "    #    The node with the highest total degree (in_degree + out_degree) is retained.\n",
    "    for lemma, nodes_same_lemma in lemma_to_nodes.items():\n",
    "        if len(nodes_same_lemma) > 1:\n",
    "            degree_dict = {node: G_total.in_degree(node) + G_total.out_degree(node)\n",
    "                           for node in nodes_same_lemma}\n",
    "            node_keep = max(degree_dict, key=degree_dict.get)\n",
    "            nodes_to_merge = [n for n in nodes_same_lemma if n != node_keep]\n",
    "            \n",
    "            for node_merge in nodes_to_merge:\n",
    "                # 1) Merge node attributes.\n",
    "                for key in set(list(G_total.nodes[node_keep].keys()) + \n",
    "                               list(G_total.nodes[node_merge].keys())):\n",
    "                    val_keep = G_total.nodes[node_keep].get(key)\n",
    "                    val_merge = G_total.nodes[node_merge].get(key)\n",
    "                    G_total.nodes[node_keep][key] = merge_values(val_keep, val_merge)\n",
    "                \n",
    "                # 2) Redirect all incoming edges from node_merge to node_keep.\n",
    "                in_edges = list(G_total.in_edges(node_merge, keys=True, data=True))\n",
    "                for u, v, key_edge, data_edge in in_edges:\n",
    "                    G_total.add_edge(u, node_keep, **data_edge)\n",
    "                \n",
    "                # 3) Redirect all outgoing edges from node_merge to node_keep.\n",
    "                out_edges = list(G_total.out_edges(node_merge, keys=True, data=True))\n",
    "                for u, v, key_edge, data_edge in out_edges:\n",
    "                    G_total.add_edge(node_keep, v, **data_edge)\n",
    "                \n",
    "                # 4) Remove the merged node.\n",
    "                G_total.remove_node(node_merge)\n",
    "    \n",
    "    return G_total\n",
    "\n",
    "\n",
    "# === Read the DataFrame ===\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('RQ3.xlsx')\n",
    "\n",
    "# Aggregate the knowledge graphs from all rows of the DataFrame.\n",
    "G_aggregated = aggregate_knowledge_graph(df, 'knowledge graph', 'keyindex')\n",
    "\n",
    "import random\n",
    "print(\"Random node and attributes:\", random.choice(list(G_aggregated.nodes(data=True))))\n",
    "print(\"Random edge and attributes:\", random.choice(list(G_aggregated.edges(data=True))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tools  # For calling tools.get_embedding interface\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#############################################\n",
    "# 1. Define Structural Feature Computation Function\n",
    "#############################################\n",
    "def compute_structural_features(G, node_pairs, max_path_length=3):\n",
    "    \"\"\"\n",
    "    Compute structural features for node pairs and return a tensor of shape [len(node_pairs), 16].\n",
    "    \"\"\"\n",
    "    print(f\"Computing structural features for {len(node_pairs)} node pairs...\")\n",
    "    features = []\n",
    "    for u, v in tqdm(node_pairs):\n",
    "        feature_vector = []\n",
    "        # 1. Shortest path (normalized as 1/(1+length))\n",
    "        try:\n",
    "            path_length = nx.shortest_path_length(G, source=u, target=v)\n",
    "            feature_vector.append(1.0 / (1.0 + path_length))\n",
    "        except:\n",
    "            feature_vector.append(0.0)\n",
    "        # 2. Number of common neighbors (normalized as log(1+count))\n",
    "        try:\n",
    "            u_neighbors = set(G.neighbors(u))\n",
    "            v_neighbors = set(G.neighbors(v))\n",
    "            common_neighbor_count = len(u_neighbors.intersection(v_neighbors))\n",
    "            feature_vector.append(math.log(1 + common_neighbor_count))\n",
    "        except:\n",
    "            feature_vector.append(0.0)\n",
    "        # 3. Jaccard coefficient\n",
    "        try:\n",
    "            union_count = len(u_neighbors.union(v_neighbors))\n",
    "            jaccard = common_neighbor_count / union_count if union_count > 0 else 0\n",
    "            feature_vector.append(jaccard)\n",
    "        except:\n",
    "            feature_vector.append(0.0)\n",
    "        # 4. Adamic-Adar index\n",
    "        try:\n",
    "            adamic_adar = sum(1.0 / math.log(G.degree(w) + 1) for w in u_neighbors.intersection(v_neighbors))\n",
    "            feature_vector.append(adamic_adar)\n",
    "        except:\n",
    "            feature_vector.append(0.0)\n",
    "        # 5. Preferential Attachment\n",
    "        try:\n",
    "            pref_attachment = G.degree(u) * G.degree(v)\n",
    "            feature_vector.append(math.log(1 + pref_attachment))\n",
    "        except:\n",
    "            feature_vector.append(0.0)\n",
    "        # 6. Resource Allocation Index\n",
    "        try:\n",
    "            resource_allocation = sum(1.0 / G.degree(w) for w in u_neighbors.intersection(v_neighbors))\n",
    "            feature_vector.append(resource_allocation)\n",
    "        except:\n",
    "            feature_vector.append(0.0)\n",
    "        # 7-9. Count of simple paths for different lengths (length = 2, 3, 4)\n",
    "        for path_len in range(2, max_path_length+1):\n",
    "            try:\n",
    "                paths = list(nx.all_simple_paths(G, source=u, target=v, cutoff=path_len))\n",
    "                path_count = len([p for p in paths if len(p)-1 == path_len])\n",
    "                feature_vector.append(math.log(1 + path_count))\n",
    "            except:\n",
    "                feature_vector.append(0.0)\n",
    "        # Pad or truncate to 16 dimensions\n",
    "        if len(feature_vector) < 16:\n",
    "            feature_vector.extend([0.0] * (16 - len(feature_vector)))\n",
    "        else:\n",
    "            feature_vector = feature_vector[:16]\n",
    "        features.append(feature_vector)\n",
    "    return torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "#############################################\n",
    "# 2. Define Node Embedding Generation Function (with caching)\n",
    "#############################################\n",
    "def get_node_embeddings_with_cache(G, cache_path, embedding_model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Generate node embeddings and return a new graph with only the 'x' attribute.\n",
    "    \"\"\"\n",
    "    # Create a copy of the graph to avoid modifying the original.\n",
    "    G_copy = nx.MultiDiGraph()\n",
    "    G_copy.add_nodes_from(G.nodes(data=True))\n",
    "    G_copy.add_edges_from([(u, v, k, d.copy()) for u, v, k, d in G.edges(data=True, keys=True)])\n",
    "    \n",
    "    embedding_cache = {}\n",
    "    cache_file = os.path.join(cache_path, f'embedding_cache_{embedding_model.replace(\"-\", \"_\")}.pkl')\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                embedding_cache = pickle.load(f)\n",
    "            print(f\"Successfully loaded embedding cache, count: {len(embedding_cache)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load cache: {e}\")\n",
    "    else:\n",
    "        print(f\"Embedding cache not found: {cache_file}\")\n",
    "    \n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    nodes_to_process = []\n",
    "    for node in G_copy.nodes():\n",
    "        node_text = str(node)\n",
    "        if node_text not in embedding_cache:\n",
    "            nodes_to_process.append(node)\n",
    "    print(f\"Number of embeddings to generate: {len(nodes_to_process)}\")\n",
    "    \n",
    "    def fetch_embedding(node):\n",
    "        try:\n",
    "            node_text = str(node)\n",
    "            emb = tools.get_embedding(node_text, model=embedding_model)\n",
    "            return node, node_text, emb, None\n",
    "        except Exception as e:\n",
    "            return node, str(node), [0.0] * 1536, str(e)\n",
    "    \n",
    "    if nodes_to_process:\n",
    "        new_entries = 0\n",
    "        with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "            futures = [executor.submit(fetch_embedding, node) for node in nodes_to_process]\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Generating embeddings\"):\n",
    "                try:\n",
    "                    node, node_text, emb, err = future.result()\n",
    "                    embedding_cache[node_text] = emb\n",
    "                    new_entries += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing node: {e}\")\n",
    "        print(f\"New embeddings added: {new_entries}\")\n",
    "        if new_entries > 0:\n",
    "            if not os.path.exists(cache_path):\n",
    "                os.makedirs(cache_path)\n",
    "            try:\n",
    "                with open(cache_file, 'wb') as f:\n",
    "                    pickle.dump(embedding_cache, f)\n",
    "                print(f\"Embedding cache updated successfully, total count: {len(embedding_cache)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to save cache: {e}\")\n",
    "    \n",
    "    # Clear node attributes and retain only 'x'\n",
    "    for node in G_copy.nodes():\n",
    "        node_text = str(node)\n",
    "        x_value = embedding_cache.get(node_text, [0.0] * 1536)\n",
    "        G_copy.nodes[node].clear()\n",
    "        G_copy.nodes[node]['x'] = x_value\n",
    "    return G_copy\n",
    "\n",
    "#############################################\n",
    "# 3. Define Negative Sample Generation Function (edges not in graph)\n",
    "#############################################\n",
    "def generate_negative_edges(G, num_neg_samples, exclude_edges=None):\n",
    "    \"\"\"\n",
    "    Generate edges that are not present in the graph as negative samples.\n",
    "    \"\"\"\n",
    "    if exclude_edges is None:\n",
    "        exclude_edges = set()\n",
    "    else:\n",
    "        exclude_edges = set(exclude_edges)\n",
    "    nodes = list(G.nodes())\n",
    "    neg_edges = []\n",
    "    pbar = tqdm(total=num_neg_samples, desc=\"Generating negative samples\")\n",
    "    while len(neg_edges) < num_neg_samples:\n",
    "        u = random.choice(nodes)\n",
    "        v = random.choice(nodes)\n",
    "        if u != v and not G.has_edge(u, v) and (u, v) not in exclude_edges:\n",
    "            neg_edges.append((u, v))\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    return neg_edges\n",
    "\n",
    "#############################################\n",
    "# 4. Split the original graph by nodes: 70% training, 30% testing (ensure no overlap to avoid data leakage)\n",
    "#############################################\n",
    "all_nodes = list(G_aggregated.nodes())\n",
    "train_nodes, test_nodes = train_test_split(all_nodes, test_size=0.3, random_state=610)\n",
    "print(f\"Number of training nodes: {len(train_nodes)}, testing nodes: {len(test_nodes)}\")\n",
    "# Construct induced subgraphs for training and testing.\n",
    "G_train_sub = G_aggregated.subgraph(train_nodes).copy()\n",
    "G_test_sub = G_aggregated.subgraph(test_nodes).copy()\n",
    "print(f\"Training subgraph - nodes: {len(G_train_sub.nodes())}, edges: {len(G_train_sub.edges())}\")\n",
    "print(f\"Testing subgraph - nodes: {len(G_test_sub.nodes())}, edges: {len(G_test_sub.edges())}\")\n",
    "\n",
    "#############################################\n",
    "# 5. Generate Node Embeddings\n",
    "#############################################\n",
    "cache_path = \"embedding_cache_path\"  # Replace with your own path\n",
    "G_train_embed = get_node_embeddings_with_cache(G_train_sub, cache_path)\n",
    "G_test_embed = get_node_embeddings_with_cache(G_test_sub, cache_path)\n",
    "\n",
    "#############################################\n",
    "# 6. Construct PyG format graph and index mapping\n",
    "#############################################\n",
    "def create_pyg_data(G):\n",
    "    # Clear edge attributes.\n",
    "    for u, v, k, d in G.edges(keys=True, data=True):\n",
    "        d.clear()\n",
    "    data = from_networkx(G, group_node_attrs=None, group_edge_attrs=None)\n",
    "    data.x = torch.tensor([G.nodes[node]['x'] for node in G.nodes()], dtype=torch.float)\n",
    "    return data\n",
    "\n",
    "data_train = create_pyg_data(G_train_embed)\n",
    "data_test = create_pyg_data(G_test_embed)\n",
    "\n",
    "train_nodes_list = list(G_train_embed.nodes())\n",
    "train_node_to_idx = {node: i for i, node in enumerate(train_nodes_list)}\n",
    "\n",
    "test_nodes_list = list(G_test_embed.nodes())\n",
    "test_node_to_idx = {node: i for i, node in enumerate(test_nodes_list)}\n",
    "\n",
    "#############################################\n",
    "# 7. Construct Positive/Negative Edges and Features\n",
    "#############################################\n",
    "def prepare_edge_data(G, node_to_idx):\n",
    "    pos_edge_pairs = [(u, v) for u, v in G.edges()]\n",
    "    pos_edge_indices = [(node_to_idx[u], node_to_idx[v]) for u, v in pos_edge_pairs]\n",
    "    pos_edge_index = torch.tensor(pos_edge_indices, dtype=torch.long).t()\n",
    "    struct = compute_structural_features(G, pos_edge_pairs)\n",
    "    return pos_edge_pairs, pos_edge_index, struct\n",
    "\n",
    "train_pos_pairs, train_pos_index, train_pos_struct = prepare_edge_data(G_train_embed, train_node_to_idx)\n",
    "test_pos_pairs, test_pos_index, test_pos_struct = prepare_edge_data(G_test_embed, test_node_to_idx)\n",
    "\n",
    "test_neg_pairs = generate_negative_edges(G_test_embed, len(test_pos_pairs))\n",
    "test_neg_indices = [(test_node_to_idx[u], test_node_to_idx[v]) for u, v in test_neg_pairs]\n",
    "test_neg_index = torch.tensor(test_neg_indices, dtype=torch.long).t()\n",
    "test_neg_struct = compute_structural_features(G_test_embed, test_neg_pairs)\n",
    "\n",
    "#############################################\n",
    "# 8. Define Model, Training and Validation Functions\n",
    "#############################################\n",
    "# 1) Model Initialization\n",
    "in_channels = data_train.x.size(1)\n",
    "hidden_channels = 128\n",
    "struct_features_dim = train_pos_struct.size(1)\n",
    "\n",
    "class AdvancedGNNLinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, struct_features_dim=16, dropout=0.3):\n",
    "        super(AdvancedGNNLinkPredictor, self).__init__()\n",
    "        \n",
    "        # GCN layer\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        # GAT layer\n",
    "        self.gat = GATConv(hidden_channels, hidden_channels, heads=4, dropout=dropout)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels * 4)\n",
    "        \n",
    "        # GraphSAGE layer\n",
    "        self.sage = SAGEConv(hidden_channels * 4, hidden_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fusion layer: fuse embeddings of two nodes with the structural features.\n",
    "        fusion_in_dim = 2 * hidden_channels + struct_features_dim\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_in_dim, hidden_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels * 2, hidden_channels)\n",
    "        )\n",
    "        \n",
    "        # Predictor layer\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, 1)\n",
    "        )\n",
    "        \n",
    "        print(f\"Model configuration: input dimension = {in_channels}, hidden dimension = {hidden_channels}, structural feature dimension = {struct_features_dim}\")\n",
    "    \n",
    "    def encode(self, x, edge_index):\n",
    "        # GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        # GAT layer\n",
    "        x = self.gat(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        # GraphSAGE layer\n",
    "        x = self.sage(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "        x = F.elu(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, z, edge_index, structural_features):\n",
    "        row, col = edge_index\n",
    "        edge_features = []\n",
    "        for i in range(edge_index.size(1)):\n",
    "            source_embedding = z[row[i]]\n",
    "            target_embedding = z[col[i]]\n",
    "            edge_struct_feature = structural_features[i]\n",
    "            combined = torch.cat([source_embedding, target_embedding, edge_struct_feature])\n",
    "            edge_feature = self.fusion(combined.unsqueeze(0)).squeeze(0)\n",
    "            edge_features.append(edge_feature)\n",
    "        edge_features = torch.stack(edge_features)\n",
    "        scores = self.predictor(edge_features)\n",
    "        return scores\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.encode(x, edge_index)\n",
    "        return z\n",
    "\n",
    "model = AdvancedGNNLinkPredictor(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    struct_features_dim=struct_features_dim,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 2) Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=True, delta=0.001, path='best_link_predictor.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} → {val_loss:.6f}), saving model...\")\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True, path=\"best_link_predictor.pt\")\n",
    "\n",
    "# 3) Training function using Hard Negative Sampling\n",
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = data_train.x.to(device)\n",
    "    edge_index = data_train.edge_index.to(device)\n",
    "    z = model(x, edge_index)\n",
    "\n",
    "    # Positive samples\n",
    "    pos_score = model.decode(z, train_pos_index.to(device), train_pos_struct.to(device))\n",
    "    num_pos = train_pos_index.size(1)\n",
    "\n",
    "    # Hard Negative Sampling\n",
    "    candidate_pool_size = num_pos * 3\n",
    "    neg_pairs = generate_negative_edges(G_train_embed, candidate_pool_size)\n",
    "    neg_indices = [(train_node_to_idx[u], train_node_to_idx[v]) for u, v in neg_pairs]\n",
    "    neg_index = torch.tensor(neg_indices, dtype=torch.long).t().to(device)\n",
    "    neg_struct = compute_structural_features(G_train_embed, neg_pairs).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        neg_scores = torch.sigmoid(model.decode(z, neg_index, neg_struct)).squeeze()\n",
    "    _, topk = torch.topk(neg_scores, k=num_pos)\n",
    "    hard_neg_index = neg_index[:, topk]\n",
    "    hard_neg_struct = neg_struct[topk]\n",
    "\n",
    "    neg_score = model.decode(z, hard_neg_index, hard_neg_struct)\n",
    "\n",
    "    edge_label = torch.cat([torch.ones(num_pos), torch.zeros(num_pos)]).to(device)\n",
    "    edge_pred = torch.cat([pos_score, neg_score]).squeeze()\n",
    "\n",
    "    loss = criterion(edge_pred, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# 4) Evaluation function on the test subgraph\n",
    "@torch.no_grad()\n",
    "def evaluate_test():\n",
    "    model.eval()\n",
    "    x = data_test.x.to(device)\n",
    "    edge_index = data_test.edge_index.to(device)\n",
    "    z = model(x, edge_index)\n",
    "    pos_score = torch.sigmoid(model.decode(z, test_pos_index.to(device), test_pos_struct.to(device))).squeeze()\n",
    "    neg_score = torch.sigmoid(model.decode(z, test_neg_index.to(device), test_neg_struct.to(device))).squeeze()\n",
    "    edge_label = torch.cat([\n",
    "        torch.ones(test_pos_index.size(1)),\n",
    "        torch.zeros(test_neg_index.size(1))\n",
    "    ]).to(device)\n",
    "    edge_pred = torch.cat([pos_score, neg_score], dim=0)\n",
    "    val_loss = criterion(edge_pred, edge_label).item()\n",
    "    auc = roc_auc_score(edge_label.cpu().numpy(), edge_pred.cpu().numpy())\n",
    "    acc = ((edge_pred >= 0.5).float() == edge_label).sum().item() / len(edge_label)\n",
    "    return val_loss, acc, auc\n",
    "\n",
    "# 5) Main training loop\n",
    "num_epochs = 300\n",
    "train_losses, test_losses, test_aucs, test_accs = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch()\n",
    "    val_loss, val_acc, val_auc = evaluate_test()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(val_loss)\n",
    "    test_aucs.append(val_auc)\n",
    "    test_accs.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: TrainLoss={train_loss:.4f}, TestLoss={val_loss:.4f}, Acc={val_acc:.4f}, AUC={val_auc:.4f}\")\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_link_predictor.pt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_test_and_typewise_5fold():\n",
    "    # Split the test positive edge set (test_pos_pairs) into 5 folds (after shuffling)\n",
    "    pos_edges = test_pos_pairs.copy()\n",
    "    random.seed(610)\n",
    "    random.shuffle(pos_edges)\n",
    "    pos_folds = np.array_split(pos_edges, 5)\n",
    "    \n",
    "    # Construct the evaluation edge set: positive edges + negative edges (negative edges are not in the graph)\n",
    "    eval_edges = test_pos_pairs + test_neg_pairs\n",
    "    # Corresponding labels: positives first, then negatives\n",
    "    eval_labels = np.array([1] * len(test_pos_pairs) + [0] * len(test_neg_pairs))\n",
    "    \n",
    "    # Variables for accumulating global statistics (across all folds)\n",
    "    total_correct_pos, total_correct_neg = 0, 0\n",
    "    total_fold = 0  # Total number of evaluation edges\n",
    "    agg_pair_stats = {}  # Accumulate statistics for each type combination\n",
    "    \n",
    "    # Lists for recording global AUC and accuracy of each fold\n",
    "    auc_list = []\n",
    "    acc_list = []\n",
    "    \n",
    "    # Collect correctly predicted edges\n",
    "    correct_pos_edges = []  # Correctly predicted positive edges (predicted as 1)\n",
    "    correct_neg_edges = []  # Correctly predicted negative edges (predicted as 0)\n",
    "    \n",
    "    # Lists for collecting ROC curve data\n",
    "    all_y_true = []\n",
    "    all_y_scores = []\n",
    "    \n",
    "    # 5-fold evaluation\n",
    "    for fold in range(5):\n",
    "        print(f\"\\n===== Fold {fold+1}/5 =====\")\n",
    "        # For the current fold, the positive edges to be removed from the test graph\n",
    "        remove_subset = list(pos_folds[fold])\n",
    "        \n",
    "        # Construct a copy of the test graph and remove the positive edges for this fold \n",
    "        # (note: negative edges are absent from the graph)\n",
    "        G_temp = G_test_embed.copy()\n",
    "        G_temp.remove_edges_from(remove_subset)\n",
    "        \n",
    "        # Compute structural features for all evaluation edges using the modified test graph\n",
    "        struct_tensor = compute_structural_features(G_temp, eval_edges).to(device)\n",
    "        \n",
    "        # Construct evaluation edge index tensor (based on test_node_to_idx)\n",
    "        # Note: test_node_to_idx should correspond to the test subgraph\n",
    "        def edge_pairs_to_tensor(edge_pairs, node_to_idx):\n",
    "            idx_pairs = [(node_to_idx[u], node_to_idx[v]) for u, v in edge_pairs]\n",
    "            return torch.tensor(idx_pairs, dtype=torch.long).t()\n",
    "        edge_index_tensor = edge_pairs_to_tensor(eval_edges, test_node_to_idx).to(device)\n",
    "        \n",
    "        # Model inference (using the global structure of the test subgraph for message passing)\n",
    "        model.eval()\n",
    "        x = data_test.x.to(device)\n",
    "        edge_index = data_test.edge_index.to(device)\n",
    "        z = model(x, edge_index)\n",
    "        \n",
    "        pos_score = torch.sigmoid(\n",
    "            model.decode(\n",
    "                z,\n",
    "                edge_index_tensor[:, :len(test_pos_pairs)].to(device),\n",
    "                struct_tensor[:len(test_pos_pairs)].to(device)\n",
    "            )\n",
    "        ).squeeze().cpu().numpy()\n",
    "        neg_score = torch.sigmoid(\n",
    "            model.decode(\n",
    "                z,\n",
    "                edge_index_tensor[:, len(test_pos_pairs):].to(device),\n",
    "                struct_tensor[len(test_pos_pairs):].to(device)\n",
    "            )\n",
    "        ).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Global predicted labels\n",
    "        y_pred = np.concatenate([(pos_score >= 0.5).astype(int), (neg_score >= 0.5).astype(int)])\n",
    "        # Global AUC and accuracy\n",
    "        y_true = np.concatenate([np.ones(len(pos_score)), np.zeros(len(neg_score))])\n",
    "        y_scores = np.concatenate([pos_score, neg_score])\n",
    "        \n",
    "        # Collect ROC data\n",
    "        all_y_true.extend(y_true)\n",
    "        all_y_scores.extend(y_scores)\n",
    "        \n",
    "        fold_auc = roc_auc_score(y_true, y_scores)\n",
    "        fold_acc = (np.sum(y_pred == y_true)) / len(y_true)\n",
    "        print(f\"[Fold {fold+1}] AUC: {fold_auc:.4f}, Acc: {fold_acc:.4f}\")\n",
    "        auc_list.append(fold_auc)\n",
    "        acc_list.append(fold_acc)\n",
    "        \n",
    "        # Accumulate the number of correctly predicted edges (positives and negatives)\n",
    "        correct_pos = sum(y_pred[:len(pos_score)] == 1)\n",
    "        correct_neg = sum(y_pred[len(pos_score):] == 0)\n",
    "        total_correct_pos += correct_pos\n",
    "        total_correct_neg += correct_neg\n",
    "        total_fold += len(eval_edges)\n",
    "        \n",
    "        # Collect correctly predicted edges\n",
    "        for i in range(len(test_pos_pairs)):\n",
    "            if y_pred[i] == 1:  # Correctly predicted positive edge\n",
    "                correct_pos_edges.append(eval_edges[i])\n",
    "        for i in range(len(test_pos_pairs), len(eval_edges)):\n",
    "            if y_pred[i] == 0:  # Correctly predicted negative edge\n",
    "                correct_neg_edges.append(eval_edges[i])\n",
    "        \n",
    "        # === Type Combination Statistics ===\n",
    "        # Define a function to extract node types from G_aggregated (excluding 'unknown')\n",
    "        def get_node_types(n):\n",
    "            if n not in G_aggregated.nodes():\n",
    "                return []\n",
    "            attr = G_aggregated.nodes[n].get(\"type\", None)\n",
    "            if isinstance(attr, list):\n",
    "                return [str(t).lower() for t in attr if t and str(t).lower() != 'unknown']\n",
    "            elif isinstance(attr, str):\n",
    "                return [attr.lower()] if attr.lower() != 'unknown' else []\n",
    "            else:\n",
    "                return []\n",
    "        \n",
    "        # For each evaluation edge (eval_edges), accumulate prediction statistics for the corresponding type combinations\n",
    "        for i, (u, v) in enumerate(eval_edges):\n",
    "            types_u = get_node_types(u)\n",
    "            types_v = get_node_types(v)\n",
    "            if not types_u or not types_v:\n",
    "                continue\n",
    "            for t1 in types_u:\n",
    "                for t2 in types_v:\n",
    "                    pair = tuple(sorted([t1, t2]))\n",
    "                    if pair not in agg_pair_stats:\n",
    "                        agg_pair_stats[pair] = {'count': 0, 'tp': 0, 'fp': 0, 'fn': 0}\n",
    "                    agg_pair_stats[pair]['count'] += 1\n",
    "                    if i < len(test_pos_pairs):  # positive example\n",
    "                        if y_pred[i] == 1:\n",
    "                            agg_pair_stats[pair]['tp'] += 1\n",
    "                        else:\n",
    "                            agg_pair_stats[pair]['fn'] += 1\n",
    "                    else:  # negative example\n",
    "                        if y_pred[i] == 1:\n",
    "                            agg_pair_stats[pair]['fp'] += 1\n",
    "\n",
    "    # Aggregate global statistics across all folds\n",
    "    global_correct_pos = total_correct_pos\n",
    "    global_correct_neg = total_correct_neg\n",
    "    global_total = total_fold\n",
    "    print(\"\\n====== 5-Fold Global Metrics ======\")\n",
    "    print(\"Number of correctly predicted positive edges:\", global_correct_pos)\n",
    "    print(\"Number of correctly predicted negative edges:\", global_correct_neg)\n",
    "    print(f\"Average AUC: {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "    print(f\"Average Accuracy: {np.mean(acc_list):.4f}\")\n",
    "    \n",
    "    # Aggregate type combination statistics into a DataFrame (based on accumulated results)\n",
    "    rows = []\n",
    "    for pair, stats in agg_pair_stats.items():\n",
    "        tp, fp, fn = stats['tp'], stats['fp'], stats['fn']\n",
    "        prec = (tp / (tp + fp) * 100) if (tp + fp) > 0 else 0\n",
    "        rec = (tp / (tp + fn) * 100) if (tp + fn) > 0 else 0\n",
    "        f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) > 0 else 0\n",
    "        rows.append({\n",
    "            \"Type Combination\": f\"{pair[0]} - {pair[1]}\",\n",
    "            \"Count\": stats['count'],\n",
    "            \"Precision\": f\"{prec:.2f}%\",\n",
    "            \"Recall\": f\"{rec:.2f}%\",\n",
    "            \"F1 Score\": f\"{f1:.2f}%\"\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sort_values(by=\"Count\", ascending=False).head(30).reset_index(drop=True)\n",
    "    \n",
    "    # Plot overall ROC curve\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    fpr, tpr, _ = roc_curve(all_y_true, all_y_scores)\n",
    "    overall_auc = roc_auc_score(all_y_true, all_y_scores)\n",
    "\n",
    "    # Use a more prominent color and thicker line width\n",
    "    plt.plot(fpr, tpr, lw=3, label=f'ROC curve (AUC = {overall_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "    # Adjust axis range to avoid overlap with borders\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "\n",
    "    # Add margins\n",
    "    plt.subplots_adjust(left=0.12, right=0.95, bottom=0.12, top=0.95)\n",
    "\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('Receiver Operating Characteristic (ROC)', fontsize=16, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\", fontsize=12)\n",
    "\n",
    "    # Add detailed grid\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "\n",
    "    # Add minor ticks on axes\n",
    "    plt.minorticks_on()\n",
    "    plt.tick_params(which='both', direction='in')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')  # Save a high resolution image\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nNumber of correctly predicted positive edges: {len(correct_pos_edges)}\")\n",
    "    print(f\"Number of correctly predicted negative edges: {len(correct_neg_edges)}\")\n",
    "    \n",
    "    return df, correct_pos_edges, correct_neg_edges, overall_auc\n",
    "\n",
    "# Call the 5-fold evaluation function\n",
    "df_result, correct_pos_edges, correct_neg_edges, overall_auc = evaluate_test_and_typewise_5fold()\n",
    "print(df_result)\n",
    "print(f\"\\nTop 10 examples of correctly predicted positive edges:\")\n",
    "for i, edge in enumerate(correct_pos_edges[:10]):\n",
    "    print(f\"{i+1}. {edge}\")\n",
    "\n",
    "print(f\"\\nTop 10 examples of correctly predicted negative edges:\")\n",
    "for i, edge in enumerate(correct_neg_edges[:10]):\n",
    "    print(f\"{i+1}. {edge}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
