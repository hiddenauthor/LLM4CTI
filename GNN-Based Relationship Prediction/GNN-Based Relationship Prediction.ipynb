{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pickle, os, json, re, time, random, logging, pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, scipy, sklearn, networkx as nx, importlib\n",
    "with open('RQ3_KnowledgeGraph.pkl', 'rb') as f:\n",
    "    G_aggregated = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#############################################\n",
    "# 1. Define function to compute structural features\n",
    "#############################################\n",
    "def compute_structural_features(G, node_pairs, max_path_length=3):\n",
    "    \"\"\"\n",
    "    Computes structural features between node pairs, including shortest path, common neighbors,\n",
    "    Jaccard, Adamic-Adar, preferential attachment, resource allocation index, and counts\n",
    "    of simple paths of different lengths.\n",
    "    Returns a feature tensor of shape [len(node_pairs), 16].\n",
    "    \"\"\"\n",
    "    print(f\"Computing structural features for {len(node_pairs)} node pairs...\")\n",
    "    features = []\n",
    "    for u, v in tqdm(node_pairs):\n",
    "        feature_vector = []\n",
    "        \n",
    "        # 1. Shortest path length (normalized: 1/(1+length))\n",
    "        try:\n",
    "            path_length = nx.shortest_path_length(G, source=u, target=v)\n",
    "            path_length_feature = 1.0 / (1.0 + path_length)\n",
    "        except:\n",
    "            path_length_feature = 0.0\n",
    "        feature_vector.append(path_length_feature)\n",
    "        \n",
    "        # 2. Common neighbors count (normalized: log(1 + count))\n",
    "        try:\n",
    "            u_neighbors = set(G.neighbors(u))\n",
    "            v_neighbors = set(G.neighbors(v))\n",
    "            common_neighbor_count = len(u_neighbors.intersection(v_neighbors))\n",
    "            common_neighbor_feature = math.log(1 + common_neighbor_count)\n",
    "        except:\n",
    "            common_neighbor_feature = 0.0\n",
    "        feature_vector.append(common_neighbor_feature)\n",
    "        \n",
    "        # 3. Jaccard coefficient\n",
    "        try:\n",
    "            union_count = len(u_neighbors.union(v_neighbors))\n",
    "            jaccard = common_neighbor_count / union_count if union_count > 0 else 0\n",
    "        except:\n",
    "            jaccard = 0.0\n",
    "        feature_vector.append(jaccard)\n",
    "        \n",
    "        # 4. Adamic-Adar index\n",
    "        try:\n",
    "            adamic_adar = sum(1.0 / math.log(G.degree(w) + 1) for w in u_neighbors.intersection(v_neighbors))\n",
    "        except:\n",
    "            adamic_adar = 0.0\n",
    "        feature_vector.append(adamic_adar)\n",
    "        \n",
    "        # 5. Preferential attachment\n",
    "        try:\n",
    "            pref_attachment = G.degree(u) * G.degree(v)\n",
    "            pref_attachment_feature = math.log(1 + pref_attachment)\n",
    "        except:\n",
    "            pref_attachment_feature = 0.0\n",
    "        feature_vector.append(pref_attachment_feature)\n",
    "        \n",
    "        # 6. Resource allocation index\n",
    "        try:\n",
    "            resource_allocation = sum(1.0 / G.degree(w) for w in u_neighbors.intersection(v_neighbors))\n",
    "        except:\n",
    "            resource_allocation = 0.0\n",
    "        feature_vector.append(resource_allocation)\n",
    "        \n",
    "        # 7-9. Simple path counts for lengths 2, 3, 4\n",
    "        for path_len in range(2, max_path_length + 1):\n",
    "            try:\n",
    "                paths = list(nx.all_simple_paths(G, source=u, target=v, cutoff=path_len))\n",
    "                path_count = len([p for p in paths if len(p) - 1 == path_len])\n",
    "                path_count_feature = math.log(1 + path_count)\n",
    "            except:\n",
    "                path_count_feature = 0.0\n",
    "            feature_vector.append(path_count_feature)\n",
    "        \n",
    "        # Pad to a fixed 16 dimensions\n",
    "        pad_length = 16\n",
    "        if len(feature_vector) < pad_length:\n",
    "            feature_vector.extend([0.0] * (pad_length - len(feature_vector)))\n",
    "        else:\n",
    "            feature_vector = feature_vector[:pad_length]\n",
    "            \n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "#############################################\n",
    "# 2. Define function to generate node embeddings (using cache)\n",
    "#############################################\n",
    "def get_node_embeddings_with_cache(G, cache_path, embedding_model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Generates node embeddings and returns a new graph where nodes only retain the 'x' attribute.\n",
    "    \"\"\"\n",
    "    G_copy = nx.MultiDiGraph()\n",
    "    G_copy.add_nodes_from(G.nodes(data=True))\n",
    "    G_copy.add_edges_from([(u, v, k, d.copy()) for u, v, k, d in G.edges(data=True, keys=True)])\n",
    "    \n",
    "    embedding_cache = {}\n",
    "    cache_file = os.path.join(cache_path, f'embedding_cache_{embedding_model.replace(\"-\", \"_\")}.pkl')\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                embedding_cache = pickle.load(f)\n",
    "            print(f\"Successfully loaded embedding cache, count: {len(embedding_cache)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load cache: {e}\")\n",
    "    else:\n",
    "        print(f\"Embedding cache not found: {cache_file}\")\n",
    "    \n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    nodes_to_process = []\n",
    "    for node in G_copy.nodes():\n",
    "        node_text = str(node)\n",
    "        if node_text not in embedding_cache:\n",
    "            nodes_to_process.append(node)\n",
    "    print(f\"Number of new embeddings to generate: {len(nodes_to_process)}\")\n",
    "    \n",
    "    def fetch_embedding(node):\n",
    "        try:\n",
    "            node_text = str(node)\n",
    "            emb = tools.get_embedding(node_text, model=embedding_model)\n",
    "            return node, node_text, emb, None\n",
    "        except Exception as e:\n",
    "            return node, str(node), [0.0]*1536, str(e)\n",
    "    \n",
    "    if nodes_to_process:\n",
    "        new_entries = 0\n",
    "        with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "            futures = [executor.submit(fetch_embedding, node) for node in nodes_to_process]\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Generating embeddings\"):\n",
    "                try:\n",
    "                    node, node_text, emb, err = future.result()\n",
    "                    embedding_cache[node_text] = emb\n",
    "                    new_entries += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing node: {e}\")\n",
    "        print(f\"Number of new embeddings added: {new_entries}\")\n",
    "        if new_entries > 0:\n",
    "            if not os.path.exists(cache_path):\n",
    "                os.makedirs(cache_path)\n",
    "            try:\n",
    "                with open(cache_file, 'wb') as f:\n",
    "                    pickle.dump(embedding_cache, f)\n",
    "                print(f\"Successfully updated embedding cache, total count: {len(embedding_cache)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to save cache: {e}\")\n",
    "    \n",
    "    # Clean node attributes, keeping only 'x'\n",
    "    for node in G_copy.nodes():\n",
    "        node_text = str(node)\n",
    "        x_value = embedding_cache.get(node_text, [0.0]*1536)\n",
    "        G_copy.nodes[node].clear()\n",
    "        G_copy.nodes[node]['x'] = x_value\n",
    "    return G_copy\n",
    "\n",
    "#############################################\n",
    "# 3. Define function to generate negative samples (edges not in the graph)\n",
    "#############################################\n",
    "def generate_negative_edges(G, num_neg_samples, exclude_edges=None):\n",
    "    \"\"\"\n",
    "    Generates edges not present in the graph as negative samples.\n",
    "    \"\"\"\n",
    "    if exclude_edges is None:\n",
    "        exclude_edges = set()\n",
    "    else:\n",
    "        exclude_edges = set(exclude_edges)\n",
    "    nodes = list(G.nodes())\n",
    "    neg_edges = []\n",
    "    pbar = tqdm(total=num_neg_samples, desc=\"Generating negative samples\")\n",
    "    while len(neg_edges) < num_neg_samples:\n",
    "        u = random.choice(nodes)\n",
    "        v = random.choice(nodes)\n",
    "        if u != v and not G.has_edge(u, v) and (u, v) not in exclude_edges:\n",
    "            neg_edges.append((u, v))\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    return neg_edges\n",
    "\n",
    "#############################################\n",
    "# 4. Split training and test sets based on the \"source\" edge attribute\n",
    "#############################################\n",
    "# Extract all sources from the edge attributes of the original graph G_aggregated\n",
    "all_edges = list(G_aggregated.edges(data=True))\n",
    "all_sources = set()\n",
    "for u, v, d in all_edges:\n",
    "    if 'source' in d:\n",
    "        all_sources.add(d['source'])\n",
    "all_sources = list(all_sources)\n",
    "random.seed(610)\n",
    "random.shuffle(all_sources)\n",
    "train_source_count = int(0.7 * len(all_sources))\n",
    "train_sources = set(all_sources[:train_source_count])\n",
    "test_sources = set(all_sources[train_source_count:])\n",
    "print(f\"Number of sources for training: {len(train_sources)}, for testing: {len(test_sources)}\")\n",
    "\n",
    "# Partition edges based on source\n",
    "train_edges = [ (u, v, d) for u, v, d in all_edges if d.get('source') in train_sources ]\n",
    "test_edges  = [ (u, v, d) for u, v, d in all_edges if d.get('source') in test_sources ]\n",
    "print(f\"Number of training edges: {len(train_edges)}, Number of test edges: {len(test_edges)}\")\n",
    "\n",
    "# Construct the training graph: extract all involved nodes from the training edges\n",
    "train_nodes=set()\n",
    "for u, v, d in train_edges:\n",
    "    train_nodes.add(u)\n",
    "    train_nodes.add(v)\n",
    "print(f\"Number of training nodes: {len(train_nodes)}\")\n",
    "G_train = nx.MultiDiGraph()\n",
    "G_train.add_nodes_from(train_nodes)\n",
    "G_train.add_edges_from(train_edges)\n",
    "\n",
    "# Construct the test graph: extract all involved nodes from the test edges\n",
    "test_nodes = set()\n",
    "for u, v, d in test_edges:\n",
    "    test_nodes.add(u)\n",
    "    test_nodes.add(v)\n",
    "print(f\"Number of test nodes: {len(test_nodes)}\")\n",
    "G_test = nx.MultiDiGraph()\n",
    "G_test.add_nodes_from(test_nodes)\n",
    "G_test.add_edges_from(test_edges)\n",
    "\n",
    "#############################################\n",
    "# 5. Generate Node Embeddings\n",
    "#############################################\n",
    "cache_path = '/path/to/embedding_cache'\n",
    "print(\"Generating node embeddings for the training graph...\")\n",
    "G_train_embed = get_node_embeddings_with_cache(G_train, cache_path, embedding_model=\"text-embedding-3-small\")\n",
    "print(\"Generating node embeddings for the test graph...\")\n",
    "G_test_embed = get_node_embeddings_with_cache(G_test, cache_path, embedding_model=\"text-embedding-3-small\")\n",
    "\n",
    "#############################################\n",
    "# 6. Convert graphs to PyG Data and create node index mappings\n",
    "#############################################\n",
    "def create_pyg_data(G_embed):\n",
    "    # Clear edge attributes (to ensure they are empty)\n",
    "    for u, v, k, d in G_embed.edges(keys=True, data=True):\n",
    "        d.clear()\n",
    "    data = from_networkx(G_embed, group_node_attrs=None, group_edge_attrs=None)\n",
    "    data.x = torch.tensor([G_embed.nodes[node]['x'] for node in G_embed.nodes()], dtype=torch.float)\n",
    "    return data\n",
    "\n",
    "data_train = create_pyg_data(G_train_embed)\n",
    "data_test = create_pyg_data(G_test_embed)\n",
    "\n",
    "# Create node-to-index mappings (independently for train and test)\n",
    "train_nodes_list = list(G_train_embed.nodes())\n",
    "train_node_to_idx = {node: i for i, node in enumerate(train_nodes_list)}\n",
    "\n",
    "test_nodes_list = list(G_test_embed.nodes())\n",
    "test_node_to_idx = {node: i for i, node in enumerate(test_nodes_list)}\n",
    "\n",
    "#############################################\n",
    "# 7. Prepare positive edge samples and structural features (separately for train and test)\n",
    "#############################################\n",
    "def prepare_edge_data(G_embed, node_to_idx):\n",
    "    pos_edge_pairs = [(u, v) for u, v in G_embed.edges()]\n",
    "    pos_edge_indices = [(node_to_idx[u], node_to_idx[v]) for u, v in pos_edge_pairs]\n",
    "    pos_edge_index = torch.tensor(pos_edge_indices, dtype=torch.long).t()\n",
    "    structural = compute_structural_features(G_embed, pos_edge_pairs)\n",
    "    return pos_edge_pairs, pos_edge_index, structural\n",
    "\n",
    "print(\"Preparing training positive samples...\")\n",
    "train_pos_edge_pairs, train_pos_edge_index, train_pos_structural = prepare_edge_data(G_train_embed, train_node_to_idx)\n",
    "print(f\"Number of training positive samples: {len(train_pos_edge_pairs)}\")\n",
    "\n",
    "print(\"Preparing test positive samples...\")\n",
    "test_pos_edge_pairs, test_pos_edge_index, test_pos_structural = prepare_edge_data(G_test_embed, test_node_to_idx)\n",
    "print(f\"Number of test positive samples: {len(test_pos_edge_pairs)}\")\n",
    "\n",
    "#############################################\n",
    "# 8. Generate negative samples for testing (via random sampling)\n",
    "#############################################\n",
    "print(\"Generating test negative samples...\")\n",
    "test_neg_edge_pairs = generate_negative_edges(G_test_embed, len(test_pos_edge_pairs))\n",
    "test_neg_edge_indices = [(test_node_to_idx[u], test_node_to_idx[v]) for u, v in test_neg_edge_pairs]\n",
    "test_neg_edge_index = torch.tensor(test_neg_edge_indices, dtype=torch.long).t()\n",
    "test_neg_structural = compute_structural_features(G_test_embed, test_neg_edge_pairs)\n",
    "print(f\"Number of test negative samples: {len(test_neg_edge_pairs)}\")\n",
    "\n",
    "#############################################\n",
    "# 9. Define the Advanced GNN Link Predictor Model\n",
    "#############################################\n",
    "class AdvancedGNNLinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, struct_features_dim=16, dropout=0.5):\n",
    "        super(AdvancedGNNLinkPredictor, self).__init__()\n",
    "        \n",
    "        # GCN layer\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        # GAT layer\n",
    "        self.gat = GATConv(hidden_channels, hidden_channels, heads=4, dropout=dropout)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels * 4)\n",
    "        \n",
    "        # GraphSAGE layer\n",
    "        self.sage = SAGEConv(hidden_channels * 4, hidden_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fusion layer: Concatenates and fuses the embeddings of two nodes with their structural features\n",
    "        fusion_in_dim = 2 * hidden_channels + struct_features_dim\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_in_dim, hidden_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels * 2, hidden_channels)\n",
    "        )\n",
    "        \n",
    "        # Prediction layer\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, 1)\n",
    "        )\n",
    "        \n",
    "        print(f\"Model configuration: Input dim = {in_channels}, Hidden dim = {hidden_channels}, Struct features dim = {struct_features_dim}\")\n",
    "    \n",
    "    def encode(self, x, edge_index):\n",
    "        # GCN\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        # GAT\n",
    "        x = self.gat(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        # GraphSAGE\n",
    "        x = self.sage(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "        x = F.elu(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, z, edge_index, structural_features):\n",
    "        row, col = edge_index\n",
    "        edge_features = []\n",
    "        for i in range(edge_index.size(1)):\n",
    "            source_embedding = z[row[i]]\n",
    "            target_embedding = z[col[i]]\n",
    "            edge_struct_feature = structural_features[i]\n",
    "            combined = torch.cat([source_embedding, target_embedding, edge_struct_feature])\n",
    "            edge_feature = self.fusion(combined.unsqueeze(0)).squeeze(0)\n",
    "            edge_features.append(edge_feature)\n",
    "        edge_features = torch.stack(edge_features)\n",
    "        scores = self.predictor(edge_features)\n",
    "        return scores\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.encode(x, edge_index)\n",
    "        return z\n",
    "\n",
    "in_channels = data_train.x.size(1)\n",
    "hidden_channels = 128\n",
    "struct_features_dim = train_pos_structural.size(1)\n",
    "\n",
    "torch.manual_seed(610)\n",
    "np.random.seed(610)\n",
    "random.seed(610)\n",
    "\n",
    "model = AdvancedGNNLinkPredictor(\n",
    "    in_channels=in_channels, \n",
    "    hidden_channels=hidden_channels,\n",
    "    struct_features_dim=struct_features_dim,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#############################################\n",
    "# 10. Define EarlyStopping\n",
    "#############################################\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, verbose=True, delta=0.001, path='best_link_predictor_inductive.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} -> {val_loss:.6f}). Saving model ...\")\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, verbose=True, delta=0.001, path='best_link_predictor_inductive.pt')\n",
    "\n",
    "#############################################\n",
    "# 11. Define the training function (with Hard Negative Sampling)\n",
    "#############################################\n",
    "def train_func():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Use the training graph\n",
    "    x = data_train.x.to(device)\n",
    "    edge_index = data_train.edge_index.to(device)\n",
    "    z = model(x, edge_index)\n",
    "    \n",
    "    # Positive sample calculation\n",
    "    pos_score = model.decode(z, train_pos_edge_index.to(device), train_pos_structural.to(device))\n",
    "    num_pos = train_pos_edge_index.size(1)\n",
    "    \n",
    "    # Generate a pool of candidate negative samples (3x the number of positive samples)\n",
    "    candidate_pool_size = num_pos * 3\n",
    "    candidate_neg_pairs = generate_negative_edges(G_train_embed, candidate_pool_size)\n",
    "    candidate_neg_indices = [(train_node_to_idx[u], train_node_to_idx[v]) for u, v in candidate_neg_pairs]\n",
    "    candidate_neg_edge_index = torch.tensor(candidate_neg_indices, dtype=torch.long).t().to(device)\n",
    "    candidate_neg_structural = compute_structural_features(G_train_embed, candidate_neg_pairs).to(device)\n",
    "    \n",
    "    # Use the current model to score candidate negative samples (without backpropagation)\n",
    "    with torch.no_grad():\n",
    "        candidate_scores = torch.sigmoid(model.decode(z, candidate_neg_edge_index, candidate_neg_structural)).squeeze()\n",
    "    # Select the highest-scoring ones as Hard Negatives (same number as positive samples)\n",
    "    _, hard_indices = torch.topk(candidate_scores, k=num_pos)\n",
    "    hard_neg_edge_index = candidate_neg_edge_index[:, hard_indices]\n",
    "    hard_neg_structural = candidate_neg_structural[hard_indices]\n",
    "    \n",
    "    neg_score = model.decode(z, hard_neg_edge_index, hard_neg_structural)\n",
    "    \n",
    "    # Construct labels (1 for positive, 0 for negative)\n",
    "    edge_label = torch.cat([torch.ones(num_pos), torch.zeros(num_pos)], dim=0).to(device)\n",
    "    edge_pred = torch.cat([pos_score, neg_score], dim=0).squeeze()\n",
    "    \n",
    "    loss = criterion(edge_pred, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "#############################################\n",
    "# 12. Define the test evaluation function (evaluate link prediction on the test graph)\n",
    "#############################################\n",
    "@torch.no_grad()\n",
    "def evaluate_test():\n",
    "    model.eval()\n",
    "    x = data_test.x.to(device)\n",
    "    edge_index = data_test.edge_index.to(device)\n",
    "    z = model(x, edge_index)\n",
    "    pos_score = torch.sigmoid(model.decode(z, test_pos_edge_index.to(device), test_pos_structural.to(device))).squeeze()\n",
    "    neg_score = torch.sigmoid(model.decode(z, test_neg_edge_index.to(device), test_neg_structural.to(device))).squeeze()\n",
    "    \n",
    "    edge_label = torch.cat([\n",
    "        torch.ones(test_pos_edge_index.size(1)),\n",
    "        torch.zeros(test_neg_edge_index.size(1))\n",
    "    ], dim=0).to(device)\n",
    "    edge_pred = torch.cat([pos_score, neg_score], dim=0)\n",
    "    \n",
    "    auc_roc = roc_auc_score(edge_label.cpu().numpy(), edge_pred.cpu().numpy())\n",
    "    preds = (edge_pred >= 0.5).float()\n",
    "    acc = (preds == edge_label).sum().item() / len(edge_label)\n",
    "    loss_val = criterion(edge_pred, edge_label).item()\n",
    "    return loss_val, acc, auc_roc\n",
    "\n",
    "#############################################\n",
    "# 13. Training Loop\n",
    "#############################################\n",
    "num_epochs = 300\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "test_aucs = []\n",
    "best_test_auc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_func()\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    t_loss, t_acc, t_auc = evaluate_test()\n",
    "    test_losses.append(t_loss)\n",
    "    test_accs.append(t_acc)\n",
    "    test_aucs.append(t_auc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss:.4f}, Test Loss: {t_loss:.4f}, Test Acc: {t_acc:.4f}, Test AUC: {t_auc:.4f}\")\n",
    "    \n",
    "    early_stopping(t_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"Training complete, Best Test AUC: {max(test_aucs):.4f}\")\n",
    "model.load_state_dict(torch.load('best_link_predictor_inductive.pt'))\n",
    "\n",
    "#############################################\n",
    "# 14. Print final evaluation results on the test set\n",
    "#############################################\n",
    "final_test_loss, final_test_acc, final_test_auc = evaluate_test()\n",
    "print(\"\\nFinal evaluation results on the test set:\")\n",
    "print(f\"Test Set - Final Loss: {final_test_loss:.4f}, Final Accuracy: {final_test_acc:.4f}, Final AUC: {final_test_auc:.4f}\")\n",
    "\n",
    "#############################################\n",
    "# 15. Plot training curves\n",
    "#############################################\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Train vs. Test Loss\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(test_accs, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Test Accuracy\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(test_aucs, label=\"Test AUC\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "plt.title(\"Test AUC\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_link_ranking_filtered(model, G_test_embed, test_pos_edge_pairs, all_true_edges_set, test_node_to_idx, data_test, device, k_values=[1, 3, 10], sample_size=None):\n",
    "    \"\"\"\n",
    "    Performs ranking evaluation (MRR, Hits@K) for the link prediction model using the strict 'Filtered' setting.\n",
    "\n",
    "    Args:\n",
    "    - model: The trained PyTorch model.\n",
    "    - G_test_embed: The test graph containing node embeddings.\n",
    "    - test_pos_edge_pairs: A list of positive edge pairs from the test set to be evaluated.\n",
    "    - all_true_edges_set: A set containing all true edges (u, v) from the entire dataset (train + test).\n",
    "    - test_node_to_idx: A dictionary mapping node names to their indices in the test set.\n",
    "    - data_test: The test graph data in PyTorch Geometric format.\n",
    "    - device: 'cuda' or 'cpu'.\n",
    "    - k_values: A list of K values for calculating Hits@K.\n",
    "    - sample_size (int, optional): The number of samples to randomly evaluate.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the evaluation results for MRR and Hits@K.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    z = model.encode(data_test.x.to(device), data_test.edge_index.to(device))\n",
    "\n",
    "    all_test_nodes = list(test_node_to_idx.keys())\n",
    "    all_test_node_indices = list(test_node_to_idx.values())\n",
    "    \n",
    "    reciprocal_ranks = []\n",
    "    hits_at_k = {k: 0 for k in k_values}\n",
    "    \n",
    "    if sample_size is not None and sample_size < len(test_pos_edge_pairs):\n",
    "        print(f\"Randomly sampling {sample_size} positive edges from {len(test_pos_edge_pairs)} for evaluation...\")\n",
    "        edges_to_evaluate = random.sample(test_pos_edge_pairs, sample_size)\n",
    "    else:\n",
    "        edges_to_evaluate = test_pos_edge_pairs\n",
    "        print(f\"Evaluating all {len(edges_to_evaluate)} positive edges...\")\n",
    "\n",
    "    for u_node, v_node_true in tqdm(edges_to_evaluate, desc=\"Filtered Link Ranking\"):\n",
    "        \n",
    "        u_idx = test_node_to_idx[u_node]\n",
    "        v_idx_true = test_node_to_idx[v_node_true]\n",
    "\n",
    "        candidate_v_nodes = all_test_nodes\n",
    "        candidate_v_indices = all_test_node_indices\n",
    "        \n",
    "        u_nodes_repeated = [u_node] * len(candidate_v_nodes)\n",
    "        batch_edge_pairs = list(zip(u_nodes_repeated, candidate_v_nodes))\n",
    "        \n",
    "        batch_edge_index = torch.tensor(\n",
    "            [[u_idx] * len(candidate_v_indices), candidate_v_indices],\n",
    "            dtype=torch.long\n",
    "        ).to(device)\n",
    "        \n",
    "        batch_struct_features = compute_structural_features(G_test_embed, batch_edge_pairs).to(device)\n",
    "        batch_scores = model.decode(z, batch_edge_index, batch_struct_features).squeeze()\n",
    "\n",
    "        true_v_position = candidate_v_indices.index(v_idx_true)\n",
    "        true_score = batch_scores[true_v_position]\n",
    "\n",
    "        # --- Core modification for the Filtered Setting ---\n",
    "        # 1. Find all other true tail entities besides the one currently being evaluated.\n",
    "        filter_indices = []\n",
    "        for i, v_cand_node in enumerate(candidate_v_nodes):\n",
    "            # If (u, v_cand) is a known true edge and v_cand is not the current v_true we are evaluating.\n",
    "            if (u_node, v_cand_node) in all_true_edges_set and v_cand_node != v_node_true:\n",
    "                filter_indices.append(i)\n",
    "        \n",
    "        # 2. Set the scores of these other true tail entities to a very low value so they don't affect the rank calculation.\n",
    "        if filter_indices:\n",
    "            batch_scores[torch.tensor(filter_indices)] = -float('inf')\n",
    "        # --- End of modification ---\n",
    "        \n",
    "        rank = (batch_scores >= true_score).sum().item()\n",
    "        \n",
    "        reciprocal_ranks.append(1.0 / rank)\n",
    "        for k in k_values:\n",
    "            if rank <= k:\n",
    "                hits_at_k[k] += 1\n",
    "\n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    for k in k_values:\n",
    "        hits_at_k[k] /= len(edges_to_evaluate)\n",
    "\n",
    "    results = {'MRR': mrr}\n",
    "    for k in k_values:\n",
    "        results[f'Hits@{k}'] = hits_at_k[k]\n",
    "        \n",
    "    return results\n",
    "\n",
    "# 1. Create a set containing all true edges.\n",
    "#    (Assumes train_pos_edge_pairs and test_pos_edge_pairs are already defined)\n",
    "all_true_edges_set = set(train_pos_edge_pairs) | set(test_pos_edge_pairs)\n",
    "\n",
    "# 2. Call the new evaluation function.\n",
    "ranking_results_filtered = evaluate_link_ranking_filtered(\n",
    "    model=model,\n",
    "    G_test_embed=G_test_embed,\n",
    "    test_pos_edge_pairs=test_pos_edge_pairs,\n",
    "    all_true_edges_set=all_true_edges_set, # Pass the set of all true edges\n",
    "    test_node_to_idx=test_node_to_idx,\n",
    "    data_test=data_test,\n",
    "    device=device,\n",
    "    k_values=[1, 3, 10],\n",
    "    sample_size=100  # Still can use sampling to speed up\n",
    ")\n",
    "\n",
    "# 3. Print the results.\n",
    "print(\"\\n===== Link Ranking Evaluation Results (Filtered Setting) =====\")\n",
    "print(f\"MRR (Mean Reciprocal Rank): {ranking_results_filtered['MRR']:.4f}\")\n",
    "for k in [1, 3, 10]:\n",
    "    print(f\"Hits@{k}: {ranking_results_filtered[f'Hits@{k}'] * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
