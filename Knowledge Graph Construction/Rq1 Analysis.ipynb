{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pickle, os, json, re, time, random, logging, pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, scipy, sklearn, networkx as nx, importlib\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel('RQ1 Evaluation.xlsx')\n",
    "\n",
    "def analyze_metrics_from_columns(df):\n",
    "    \"\"\"\n",
    "    Automatically identify columns with '_Precision' and '_Recall',\n",
    "    compute precision and recall for each row,\n",
    "    then calculate the average across all rows.\n",
    "    \"\"\"\n",
    "    # Clean column names by removing '\\n'\n",
    "    df.columns = [col.strip().replace('\\n', '') for col in df.columns]\n",
    "    \n",
    "    # Identify all model names (based on \"_Precision\" and \"_Recall\" columns)\n",
    "    precision_cols = [col for col in df.columns if '_Precision' in col]\n",
    "    recall_cols = [col for col in df.columns if '_Recall' in col]\n",
    "    \n",
    "    models = set()\n",
    "    for col in precision_cols + recall_cols:\n",
    "        model_name = col.split('_')[0]\n",
    "        if model_name:\n",
    "            models.add(model_name)\n",
    "    \n",
    "    print(f\"Detected models: {', '.join(models)}\")\n",
    "    print(f\"Precision columns: {', '.join(precision_cols)}\")\n",
    "    print(f\"Recall columns: {', '.join(recall_cols)}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model in models:\n",
    "        precision_col = f'{model}_Precision'\n",
    "        recall_col = f'{model}_Recall'\n",
    "        \n",
    "        if precision_col not in df.columns:\n",
    "            print(f\"Warning: Precision column for {model} not found\")\n",
    "            continue\n",
    "        if recall_col not in df.columns:\n",
    "            print(f\"Warning: Recall column for {model} not found\")\n",
    "            continue\n",
    "            \n",
    "        precision_values = []\n",
    "        recall_values = []\n",
    "        f1_values = []\n",
    "        \n",
    "        total_tp = 0\n",
    "        total_fp = 0\n",
    "        total_found = 0\n",
    "        total_miss = 0\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            if pd.notna(row[precision_col]) and isinstance(row[precision_col], str):\n",
    "                cell_lower = row[precision_col].lower()\n",
    "                tp = cell_lower.count('#tp')\n",
    "                fp = cell_lower.count('#fp')\n",
    "                total_tp += tp\n",
    "                total_fp += fp\n",
    "                if tp + fp > 0:\n",
    "                    precision_values.append(tp / (tp + fp))\n",
    "            \n",
    "            if pd.notna(row[recall_col]) and isinstance(row[recall_col], str):\n",
    "                cell_lower = row[recall_col].lower()\n",
    "                found = cell_lower.count('#found')\n",
    "                miss = cell_lower.count('#miss')\n",
    "                total_found += found\n",
    "                total_miss += miss\n",
    "                if found + miss > 0:\n",
    "                    recall_values.append(found / (found + miss))\n",
    "        \n",
    "        avg_precision = np.mean(precision_values) if precision_values else 0\n",
    "        avg_recall = np.mean(recall_values) if recall_values else 0\n",
    "        \n",
    "        for p, r in zip(precision_values, recall_values):\n",
    "            if p + r > 0:\n",
    "                f1_values.append(2 * (p * r) / (p + r))\n",
    "        \n",
    "        avg_f1 = np.mean(f1_values) if f1_values else 0\n",
    "        \n",
    "        results[model] = {\n",
    "            'TP': total_tp,\n",
    "            'FP': total_fp,\n",
    "            'Avg_Precision': avg_precision,\n",
    "            'Found': total_found,\n",
    "            'Miss': total_miss,\n",
    "            'Avg_Recall': avg_recall,\n",
    "            'Avg_F1': avg_f1,\n",
    "            'Row_Count': len(precision_values)\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "# Run the analysis\n",
    "results_df = analyze_metrics_from_columns(df)\n",
    "print(\"\\nPrecision and Recall statistics for each model (averaged by rows):\")\n",
    "print(results_df)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Precision comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(results_df.index, results_df['Avg_Precision'], color='skyblue')\n",
    "plt.title('Average Precision per Model')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Average Precision')\n",
    "\n",
    "# Recall comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(results_df.index, results_df['Avg_Recall'], color='lightgreen')\n",
    "plt.title('Average Recall per Model')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Average Recall')\n",
    "\n",
    "# F1 score comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(results_df.index, results_df['Avg_F1'], color='salmon')\n",
    "plt.title('Average F1 Score per Model')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Average F1 Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def plot_metrics_by_token_length(df, bin_size=250, fig_width=10, fig_height=10, font_size=10):\n",
    "    \"\"\"\n",
    "    Plot precision and recall curves across different token lengths\n",
    "    \"\"\"\n",
    "    token_col = None\n",
    "    for col in df.columns:\n",
    "        if 'token' in col.lower() and 'length' in col.lower():\n",
    "            token_col = col\n",
    "            break\n",
    "\n",
    "    if token_col is None:\n",
    "        print(\"Error: Token length column not found!\")\n",
    "        return\n",
    "\n",
    "    print(f\"Using '{token_col}' as the token length data...\")\n",
    "\n",
    "    methods = [\"LLM4CTI\", \"CTIKG\", \"GPT4o\", \"extractor\"]\n",
    "    rename_map = {\n",
    "        \"LLM4CTI\": \"LLM4CTI\",\n",
    "        \"CTIKG\": \"CTIKG\",\n",
    "        \"GPT4o\": \"GPT4o\",\n",
    "        \"extractor\": \"Extractor\"\n",
    "    }\n",
    "\n",
    "    df = df.dropna(subset=[token_col])\n",
    "    df[token_col] = pd.to_numeric(df[token_col], errors='coerce')\n",
    "    df = df.dropna(subset=[token_col])\n",
    "\n",
    "    def bin_label(length):\n",
    "        if length > 2000:\n",
    "            return \"2000+\"\n",
    "        else:\n",
    "            return str(int((length // bin_size) * bin_size + (bin_size / 2)))\n",
    "\n",
    "    df['token_bin_label'] = df[token_col].apply(bin_label)\n",
    "\n",
    "    plt.rcParams['font.size'] = font_size\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(fig_width, fig_height))\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "    precision_handles = []\n",
    "    precision_labels = []\n",
    "    recall_handles = []\n",
    "    recall_labels = []\n",
    "    \n",
    "    precision_data = {}\n",
    "    recall_data = {}\n",
    "\n",
    "    for i, method in enumerate(methods):\n",
    "        p_col = method + \"_Precision\"\n",
    "        r_col = method + \"_Recall\"\n",
    "\n",
    "        if p_col not in df.columns or r_col not in df.columns:\n",
    "            print(f\"Warning: Missing {p_col} or {r_col}, skipping {method}\")\n",
    "            continue\n",
    "\n",
    "        bin_precision = {}\n",
    "        bin_recall = {}\n",
    "        bin_samples = {}\n",
    "\n",
    "        for bin_val, bin_df in df.groupby('token_bin_label'):\n",
    "            tp = fp = found = miss = 0\n",
    "\n",
    "            for _, row in bin_df.iterrows():\n",
    "                if pd.notna(row.get(p_col)) and isinstance(row[p_col], str):\n",
    "                    cell = row[p_col].lower()\n",
    "                    tp += cell.count('#tp')\n",
    "                    fp += cell.count('#fp')\n",
    "\n",
    "                if pd.notna(row.get(r_col)) and isinstance(row[r_col], str):\n",
    "                    cell = row[r_col].lower()\n",
    "                    found += cell.count('#found')\n",
    "                    miss += cell.count('#miss')\n",
    "\n",
    "            if tp + fp > 0:\n",
    "                bin_precision[bin_val] = tp / (tp + fp)\n",
    "\n",
    "            if found + miss > 0:\n",
    "                bin_recall[bin_val] = found / (found + miss)\n",
    "\n",
    "            bin_samples[bin_val] = bin_df.shape[0]\n",
    "\n",
    "        def sort_key(x):\n",
    "            return float(x.replace('+', '')) if '+' not in x else 9999\n",
    "\n",
    "        p_bins = sorted(bin_precision.keys(), key=sort_key)\n",
    "        p_values = [bin_precision[b] for b in p_bins]\n",
    "\n",
    "        r_bins = sorted(bin_recall.keys(), key=sort_key)\n",
    "        r_values = [bin_recall[b] for b in r_bins]\n",
    "\n",
    "        display_name = rename_map.get(method, method)\n",
    "        print(f\"\\nMethod: {display_name}\")\n",
    "        print(f\"  Precision bins: {len(p_bins)}, total samples: {sum([bin_samples.get(b, 0) for b in p_bins])}\")\n",
    "        print(f\"  Recall bins: {len(r_bins)}, total samples: {sum([bin_samples.get(b, 0) for b in r_bins])}\")\n",
    "\n",
    "        precision_data[display_name] = {bin_val: bin_precision.get(bin_val, np.nan) for bin_val in p_bins}\n",
    "        recall_data[display_name] = {bin_val: bin_recall.get(bin_val, np.nan) for bin_val in r_bins}\n",
    "\n",
    "        if p_bins:\n",
    "            line, = ax1.plot(p_bins, p_values, '-', color=colors[i % len(colors)], label=display_name, alpha=0.8)\n",
    "            ax1.scatter(p_bins, p_values, color=colors[i % len(colors)], s=30, alpha=0.7)\n",
    "            precision_handles.append(line)\n",
    "            precision_labels.append(display_name)\n",
    "\n",
    "        if r_bins:\n",
    "            line, = ax2.plot(r_bins, r_values, '-', color=colors[i % len(colors)], label=display_name, alpha=0.8)\n",
    "            ax2.scatter(r_bins, r_values, color=colors[i % len(colors)], s=30, alpha=0.7)\n",
    "            recall_handles.append(line)\n",
    "            recall_labels.append(display_name)\n",
    "\n",
    "    ax1.set_title(f'Precision by Token Length (Binned by {bin_size} Tokens)')\n",
    "    ax1.set_xlabel('Token Length')\n",
    "    ax1.set_ylabel('Precision')\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax1.set_ylim(0, 1.05)\n",
    "    ax1.legend(precision_handles, precision_labels, loc='best')\n",
    "\n",
    "    ax2.set_title(f'Recall by Token Length (Binned by {bin_size} Tokens)')\n",
    "    ax2.set_xlabel('Token Length')\n",
    "    ax2.set_ylabel('Recall')\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    ax2.legend(recall_handles, recall_labels, loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    precision_df = pd.DataFrame(precision_data)\n",
    "    recall_df = pd.DataFrame(recall_data)\n",
    "    \n",
    "    return fig, precision_df, recall_df\n",
    "\n",
    "# Run plotting\n",
    "fig, precision_df, recall_df = plot_metrics_by_token_length(df, bin_size=250, fig_width=6, fig_height=10)\n",
    "fig.savefig('rq1.jpg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(\"\\nPrecision DataFrame:\")\n",
    "print(precision_df)\n",
    "print(\"\\nRecall DataFrame:\")\n",
    "print(recall_df)\n",
    "\n",
    "# --- The last part about grouping by \"Article Type\" ---\n",
    "methods = [\"LLM4CTI\", \"CTIKG\", \"GPT4o\", \"Extractor\"]\n",
    "rename_map = {\n",
    "    \"LLM4CTI\": \"LLM4CTI\",\n",
    "    \"CTIKG\": \"CTIKG\",\n",
    "    \"GPT4o\": \"GPT4o\",\n",
    "    \"Extractor\": \"Extractor\"\n",
    "}\n",
    "\n",
    "input_types = sorted(df[\"Aritcle Type\"].drop_duplicates().tolist())\n",
    "\n",
    "rows = []\n",
    "for classification, subdf in df.groupby(\"Aritcle Type\", sort=False):\n",
    "    for method in methods:\n",
    "        p_col = method + \"_Precision\"\n",
    "        r_col = method + \"_Recall\"\n",
    "        tp = fp = found = miss = 0\n",
    "        \n",
    "        for _, row in subdf.iterrows():\n",
    "            if pd.notna(row.get(p_col)) and isinstance(row[p_col], str):\n",
    "                cell = row[p_col].lower()\n",
    "                tp += cell.count('#tp')\n",
    "                fp += cell.count('#fp')\n",
    "            if pd.notna(row.get(r_col)) and isinstance(row[r_col], str):\n",
    "                cell = row[r_col].lower()\n",
    "                found += cell.count('#found')\n",
    "                miss += cell.count('#miss')\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "        recall = found / (found + miss) if (found + miss) else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "        \n",
    "        rows.append({\n",
    "            \"Input Type\": classification,\n",
    "            \"Method\": method,\n",
    "            \"Precision\": precision * 100,\n",
    "            \"Recall\": recall * 100,\n",
    "            \"F1\": f1 * 100\n",
    "        })\n",
    "\n",
    "df_long = pd.DataFrame(rows)\n",
    "\n",
    "precision_pivot = df_long.pivot(index=\"Input Type\", columns=\"Method\", values=\"Precision\")\n",
    "recall_pivot    = df_long.pivot(index=\"Input Type\", columns=\"Method\", values=\"Recall\")\n",
    "f1_pivot        = df_long.pivot(index=\"Input Type\", columns=\"Method\", values=\"F1\")\n",
    "\n",
    "df_combined = pd.concat(\n",
    "    [precision_pivot, recall_pivot, f1_pivot],\n",
    "    axis=1,\n",
    "    keys=[\"Precision\", \"Recall\", \"F1\"]\n",
    ")\n",
    "\n",
    "df_combined = df_combined.swaplevel(axis=1)\n",
    "\n",
    "df_combined.rename(columns=rename_map, level=0, inplace=True)\n",
    "\n",
    "desired_outer = [rename_map[m] for m in methods]\n",
    "df_combined = df_combined.reindex(desired_outer, axis=1, level=0)\n",
    "\n",
    "desired_inner = [\"Precision\", \"Recall\", \"F1\"]\n",
    "df_combined = df_combined.reindex(desired_inner, axis=1, level=1)\n",
    "\n",
    "df_combined = df_combined.reindex(sorted(df_combined.index))\n",
    "\n",
    "zero_rows = df_combined.apply(lambda row: (row == 0).all(), axis=1)\n",
    "if zero_rows.any():\n",
    "    print(f\"Removed rows with all zero values: {df_combined[zero_rows].index.tolist()}\")\n",
    "    df_combined = df_combined[~zero_rows]\n",
    "\n",
    "df_avg = df_combined.mean(axis=0, numeric_only=True).to_frame().T\n",
    "df_avg.index = [\"Average\"]\n",
    "\n",
    "df_final = pd.concat([df_combined, df_avg], axis=0)\n",
    "\n",
    "all_indices = df_final.index.tolist()\n",
    "if \"Average\" in all_indices:\n",
    "    all_indices.remove(\"Average\")\n",
    "all_indices = sorted(all_indices)\n",
    "all_indices.append(\"Average\")\n",
    "df_final = df_final.reindex(all_indices)\n",
    "\n",
    "def format_val(x):\n",
    "    return f\"{x:.2f}%\" if pd.notnull(x) else \"\"\n",
    "\n",
    "df_final = df_final.applymap(format_val)\n",
    "\n",
    "df_final\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
